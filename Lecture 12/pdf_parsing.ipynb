{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in d:\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: pymupdf in d:\\anaconda3\\lib\\site-packages (1.25.1)\n",
      "Requirement already satisfied: llama-index-core in d:\\anaconda3\\lib\\site-packages (0.11.23)\n",
      "Requirement already satisfied: llama-parse in d:\\anaconda3\\lib\\site-packages (0.5.14)\n",
      "Requirement already satisfied: llama-index-readers-file in d:\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: langchain-huggingface in d:\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: marker-pdf in d:\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: cryptography in d:\\anaconda3\\lib\\site-packages (from pypdf[full]) (43.0.3)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in d:\\anaconda3\\lib\\site-packages (from pypdf[full]) (10.4.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in d:\\anaconda3\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (2024.10.0)\n",
      "Requirement already satisfied: httpx in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (3.3)\n",
      "Requirement already satisfied: nltk>3.8.1 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (2.10.4)\n",
      "Requirement already satisfied: requests>=2.31.0 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in d:\\anaconda3\\lib\\site-packages (from llama-index-core) (1.16.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in d:\\anaconda3\\lib\\site-packages (from llama-parse) (8.1.7)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in d:\\anaconda3\\lib\\site-packages (from llama-index-readers-file) (4.12.3)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\lib\\site-packages (from llama-index-readers-file) (2.2.2)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in d:\\anaconda3\\lib\\site-packages (from llama-index-readers-file) (0.0.26)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in d:\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.26.2)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in d:\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.3.25)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in d:\\anaconda3\\lib\\site-packages (from langchain-huggingface) (3.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in d:\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.20.3)\n",
      "Requirement already satisfied: transformers>=4.39.0 in d:\\anaconda3\\lib\\site-packages (from langchain-huggingface) (4.46.2)\n",
      "Requirement already satisfied: ftfy<7.0.0,>=6.1.1 in d:\\anaconda3\\lib\\site-packages (from marker-pdf) (6.3.1)\n",
      "Requirement already satisfied: markdownify<0.14.0,>=0.13.1 in d:\\anaconda3\\lib\\site-packages (from marker-pdf) (0.13.1)\n",
      "Requirement already satisfied: pdftext<0.5.0,>=0.4.0 in d:\\anaconda3\\lib\\site-packages (from marker-pdf) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.0.3 in d:\\anaconda3\\lib\\site-packages (from marker-pdf) (2.6.1)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from marker-pdf) (1.0.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.1 in d:\\anaconda3\\lib\\site-packages (from marker-pdf) (3.10.1)\n",
      "Requirement already satisfied: regex<2025.0.0,>=2024.4.28 in d:\\anaconda3\\lib\\site-packages (from marker-pdf) (2024.9.11)\n",
      "Requirement already satisfied: surya-ocr<0.9.0,>=0.8.0 in d:\\anaconda3\\lib\\site-packages (from marker-pdf) (0.8.1)\n",
      "Requirement already satisfied: tabled-pdf<0.3.0,>=0.2.0 in d:\\anaconda3\\lib\\site-packages (from marker-pdf) (0.2.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in d:\\anaconda3\\lib\\site-packages (from marker-pdf) (0.9.0)\n",
      "Requirement already satisfied: texify<0.3.0,>=0.2.1 in d:\\anaconda3\\lib\\site-packages (from marker-pdf) (0.2.1)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.5.1 in d:\\anaconda3\\lib\\site-packages (from marker-pdf) (2.5.1+cu124)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in d:\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.15.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.5)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from click<9.0.0,>=8.1.7->llama-parse) (0.4.6)\n",
      "Requirement already satisfied: wcwidth in d:\\anaconda3\\lib\\site-packages (from ftfy<7.0.0,>=6.1.1->marker-pdf) (0.2.13)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in d:\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.1.142)\n",
      "Requirement already satisfied: six<2,>=1.15 in d:\\anaconda3\\lib\\site-packages (from markdownify<0.14.0,>=0.13.1->marker-pdf) (1.16.0)\n",
      "Requirement already satisfied: joblib in d:\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
      "Requirement already satisfied: pypdfium2<5.0.0,>=4.29.0 in d:\\anaconda3\\lib\\site-packages (from pdftext<0.5.0,>=0.4.0->marker-pdf) (4.30.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (2024.8.30)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.0)\n",
      "Requirement already satisfied: scipy in d:\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\anaconda3\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
      "Requirement already satisfied: opencv-python<5.0.0.0,>=4.9.0.80 in d:\\anaconda3\\lib\\site-packages (from surya-ocr<0.9.0,>=0.8.0->marker-pdf) (4.10.0.84)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.5.1->marker-pdf) (3.1.4)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.5.1->marker-pdf) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\anaconda3\\lib\\site-packages (from torch<3.0.0,>=2.5.1->marker-pdf) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch<3.0.0,>=2.5.1->marker-pdf) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\anaconda3\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in d:\\anaconda3\\lib\\site-packages (from cryptography->pypdf[full]) (1.17.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core) (3.22.0)\n",
      "Requirement already satisfied: anyio in d:\\anaconda3\\lib\\site-packages (from httpx->llama-index-core) (4.5.2)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda3\\lib\\site-packages (from httpx->llama-index-core) (1.0.6)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda3\\lib\\site-packages (from httpx->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda3\\lib\\site-packages (from pandas->llama-index-readers-file) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\lib\\site-packages (from pandas->llama-index-readers-file) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda3\\lib\\site-packages (from pandas->llama-index-readers-file) (2024.2)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography->pypdf[full]) (2.22)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\anaconda3\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\lib\\site-packages (from jinja2->torch<3.0.0,>=2.5.1->marker-pdf) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf pypdf[full] pymupdf llama-index-core llama-parse llama-index-readers-file langchain-huggingface marker-pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pdf Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q, K, V) = softmax(QKT\n",
      "√dk\n",
      ")V (1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of 1√dk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n",
      ".\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneficial to linearly project the queries, keys and values h times with different, learned\n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      "variables with mean 0 and variance 1. Then their dot product, q · k = Pdk\n",
      "i=1 qiki, has mean 0 and variance dk.\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "reader = PdfReader(\"attention.pdf\")\n",
    "page = reader.pages[3]\n",
    "print(page.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rotated text discovered. Output will be incomplete.\n"
     ]
    }
   ],
   "source": [
    "documents_naive = []\n",
    "for i in range(len(reader.pages)):\n",
    "    doc = reader.pages[i].extract_text(extraction_mode=\"layout\") #保留原有的文档结构\n",
    "    documents_naive.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Scaled Dot-Product Attention                                     Multi-Head Attention\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figure 2:  (left) Scaled Dot-Product Attention.  (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "\n",
      "\n",
      "ofthe values, wherethe weightassigned toeach valueis computedby acompatibility functionof the\n",
      "query with the corresponding key.\n",
      "\n",
      "3.2.1   Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queriesand keysof dimensiondk,and valuesof dimensiondv. Wecomputethe dotproducts ofthe\n",
      "querywith allkeys, divideeach by√            dk,and applya softmaxfunctionto obtaintheweights onthe\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "intoamatrixQ . ThekeysandvaluesarealsopackedtogetherintomatricesK   andV  . Wecompute\n",
      "the matrix of outputs as:\n",
      "\n",
      "\n",
      "                                  Attention(      Q,K,V     ) = softmax(        QK    T√dk  )V                                          (1)\n",
      "\n",
      "Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of    1√dk . Additiveattentioncomputes thecompatibility functionusinga feed-forwardnetwork with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "muchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized\n",
      "matrix multiplication code.\n",
      "Whileforsmallvaluesofdk thetwomechanismsperformsimilarly,additiveattentionoutperforms\n",
      "dot product attention without scaling for larger values ofdk [3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients 4. To counteract this effect, we scale the dot products by    1√             dk .\n",
      "\n",
      "3.2.2   Multi-Head Attention\n",
      "Insteadofperformingasingleattentionfunctionwithd model-dimensionalkeys,valuesandqueries,\n",
      "wefounditbeneficialtolinearlyprojectthequeries,keysandvaluesh  timeswithdifferent,learned\n",
      "linear projectionstodk,dk anddv dimensions, respectively. On eachof these projected versionsof\n",
      "queries, keysand values wethen perform theattention function inparallel, yieldingdv-dimensional\n",
      "\n",
      "   4To illustratewhythe dotproducts getlarge,assumethat thecomponents ofq andk  areindependent random\n",
      "variables withmean 0  and variance 1 . Then theirdot product,q   · k  = P  d ki=1   q ik i, hasmean 0  and varianced k .\n",
      "\n",
      "\n",
      "                                                           4\n"
     ]
    }
   ],
   "source": [
    "# extract text in a fixed width format that closely adheres to the rendered\n",
    "# layout in the source pdf\n",
    "print(page.extract_text(extraction_mode=\"layout\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: page_23_image_1.jpeg\n"
     ]
    }
   ],
   "source": [
    "import fitz \n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# 提取文件中的图片\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = \"report.pdf\"\n",
    "pdf = fitz.open(pdf_document)\n",
    "\n",
    "# Iterate through each page\n",
    "for page_number in range(len(pdf)):\n",
    "    page = pdf.load_page(page_number)\n",
    "    images = page.get_images(full=True)\n",
    "\n",
    "    # Iterate through each image on the page\n",
    "    for img_index, img in enumerate(images):\n",
    "        xref = img[0]\n",
    "        base_image = pdf.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]\n",
    "        image_ext = base_image[\"ext\"]\n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "        # Save the image\n",
    "        image_filename = f\"page_{page_number +· 1}_image_{img_index + 1}.{image_ext}\"\n",
    "        image.save(open(image_filename, \"wb\"))\n",
    "\n",
    "        print(f\"Saved image: {image_filename}\")\n",
    "\n",
    "pdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 340d1345-9f18-46d4-97eb-64e0cfa6cfc8\n",
      "[Document(id_='4c58c251-400d-4799-a175-9135353e8364', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\\n\\n# Attention Is All You Need\\n\\narXiv:1706.03762v7 · [cs.CL] · 2 Aug 2023\\n\\nAshish Vaswani∗, Noam Shazeer∗, Niki Parmar∗, Jakob Uszkoreit∗\\n\\nGoogle Brain, Google Brain, Google Research, Google Research\\n\\navaswani@google.com, noam@google.com, nikip@google.com, usz@google.com\\n\\nLlion Jones∗, Aidan N. Gomez∗ †, Łukasz Kaiser∗\\n\\nGoogle Research, University of Toronto, Google Brain\\n\\nllion@google.com, aidan@cs.toronto.edu, lukaszkaiser@google.com\\n\\nIllia Polosukhin∗ ‡\\n\\nillia.polosukhin@gmail.com\\n\\n# Abstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\\n\\n†Work performed while at Google Brain.\\n\\n‡Work performed while at Google Research.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='df1dd043-f433-4482-9411-2f32876b5b2b', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# 1 Introduction\\n\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n# 2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\\n\\n# 3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2f4dcd4d-a93d-4757-8b06-019a49cab2d1', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Figure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n# 3.1 Encoder and Decoder Stacks\\n\\n# Encoder:\\n\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\n\\n# Decoder:\\n\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\\n\\n# 3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f7b5974b-f5d5-4a10-bf80-f9cbb42b78d8', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Scaled Dot-Product Attention\\n\\n# Multi-Head Attention\\n\\n| |Linear|Concat|\\n|---|---|---|\\n|MatMul| |Scaled Dot-Product Attention|\\n|SoftMax|Mask (opt)| |\\n|Scale|MatMul|Linear|\\n| |Linear|Linear|\\n\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n# 3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values.\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:\\n\\nAttention(Q, K, V) = softmax(√dkQKT)V (1)\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi- multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of √1/dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4. To counteract this effect, we scale the dot products by √1/dk.\\n\\n# 3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional variables with mean 0 and variance 1. Then their dot product, q · k = ∑i=1dk qiki, has mean 0 and variance dk.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='cbd8d827-4df5-4744-a675-68d67b918c65', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)WO\\n\\nwhere headi = Attention(QWiQ, KWiK, VWiV)\\n\\nWhere the projections are parameter matrices Wi ∈ Rdmodel×dk, Wi ∈ Rdmodel×dk, Wi ∈ Rdmodel×dv and WO ∈ Rhdv×dmodel.\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n# 3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\\n- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\\n\\n# 3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.\\n\\n# 3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9e4445f7-5914-4566-9b7f-c6a146188a7f', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\\n\\n|Layer Type|Complexity per Layer|Sequential Operations|Maximum Path Length|\\n|---|---|---|---|\\n|Self-Attention|O(n2 · d)|O(1)|O(1)|\\n|Recurrent|O(k · n · d)|O(n)|O(logk(n))|\\n|Convolutional|O(r · n · d)|O(1)|O(n/r)|\\n\\n# 3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\nPE(pos, 2i) = sin(pos/100002i/dmodel)\\n\\nPE(pos, 2i+1) = cos(pos/100002i/dmodel)\\n\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos).\\n\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n# 4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b90180d0-04ed-4e00-b446-49bf7d8403c8', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width k &lt; n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n# 5 Training\\n\\nThis section describes the training regime for our models.\\n\\n# 5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n# 5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, (described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n# 5.3 Optimizer\\n\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning rate over the course of training, according to the formula:\\n\\nlrate = dmodel−0.5· min(step_num−0.5, step_num · warmup_steps−1.5)\\n\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\\n\\n# 5.4 Regularization\\n\\nWe employ three types of regularization during training:', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4b04a4a2-5f30-4cff-b098-a7f55f6fd15d', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='**Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.**\\n|Model|EN-DE|EN-FR|Training Cost (FLOPs)| |\\n|---|---|---|---|---|\\n|ByteNet [18]|23.75| | | |\\n|Deep-Att + PosUnk [39]| |39.2|1.0 · 1020| |\\n|GNMT + RL [38]|24.6|39.92|2.3 · 1018|1.4 · 1020|\\n|ConvS2S [9]|25.16|40.46|9.6 · 1019|1.5 · 1020|\\n|MoE [32]|26.03|40.56|2.0 · 10|1.2 · 10|\\n|Deep-Att + PosUnk Ensemble [39]| |40.4|8.0 · 1021| |\\n|GNMT + RL Ensemble [38]|26.30|41.16|1.8 · 1019|1.1 · 1021|\\n|ConvS2S Ensemble [9]|26.36|41.29|7.7 · 10|1.2 · 10|\\n|Transformer (base model)|27.3|38.1|3.3 · 1018| |\\n|Transformer (big)|28.4|41.8|2.3 · 10| |\\n\\nResidual Dropout\\n\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\\n\\nLabel Smoothing\\n\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n# 6 Results\\n\\n# 6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\\n\\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU5.\\n\\n# 6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\\n\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='961cdb26-d13e-4c61-8239-849181b163f9', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Table 3: Variations on the Transformer architecture.\\n\\nUnlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\n|N|dmodel|dff|h|dk|dv|Pdrop|ϵls|train steps|PPL (dev)|BLEU (dev)|params ×106| |\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|base|6|512|2048|8|64|64|0.1|0.1|100K|4.92|25.8|65|\\n|(A)|1|512|512| | | | | |5.29|24.9| | |\\n| |4|128|128| | | | | |5.00|25.5| | |\\n| |16|32|32| | | | | |4.91|25.8| | |\\n| |32|16|16| | | | | |5.01|25.4| | |\\n|(B)| |16| | | | | | |5.16|25.1|58| |\\n| | |32| | | | | | |5.01|25.4|60| |\\n| |2| | | | | | | |6.11|23.7|36| |\\n| |4| | | | | | | |5.19|25.3|50| |\\n| |8| | | | | | | |4.88|25.5|80| |\\n|(C)|256| |32|32| | | | |5.75|24.5|28| |\\n| |1024| |128|128| | | | |4.66|26.0|168| |\\n| | |1024| | | | | | |5.12|25.4|53| |\\n| | |4096| | | | | | |4.75|26.2|90| |\\n|(D)| | | | | |0.0| | |5.77|24.6| | |\\n| | | | | | |0.2| | |4.95|25.5| | |\\n| | | | | | |0.0| | |4.67|25.3| | |\\n| | | | | | |0.2| | |5.47|25.7| | |\\n|(E)| | | | | |positional embedding instead of sinusoids| | |4.92|25.7| | |\\n|big|6|1024|4096|16| |0.3| |300K|4.33|26.4|213| |\\n\\nWe used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\\n\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\\n\\n# 6.3 English Constituency Parsing\\n\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\\n\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\\n\\nWe performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='037e340e-d92b-4795-84e4-8587ac406e6a', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\\n\\n|Parser|Training|WSJ 23 F1|\\n|---|---|---|\\n|Vinyals & Kaiser el al. (2014) [37]|WSJ only, discriminative|88.3|\\n|Petrov et al. (2006) [29]|WSJ only, discriminative|90.4|\\n|Zhu et al. (2013) [40]|WSJ only, discriminative|90.4|\\n|Dyer et al. (2016) [8]|WSJ only, discriminative|91.7|\\n|Transformer (4 layers)|WSJ only, discriminative|91.3|\\n|Zhu et al. (2013) [40]|semi-supervised|91.3|\\n|Huang & Harper (2009) [14]|semi-supervised|91.3|\\n|McClosky et al. (2006) [26]|semi-supervised|92.1|\\n|Vinyals & Kaiser el al. (2014) [37]|semi-supervised|92.1|\\n|Transformer (4 layers)|semi-supervised|92.7|\\n|Luong et al. (2015) [23]|multi-task|93.0|\\n|Dyer et al. (2016) [8]|generative|93.3|\\n\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting.\\n\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\\n\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\\n\\n# 7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n\\nThe code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\\n\\n# Acknowledgements\\n\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\n# References\\n\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\\n\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\\n\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8b698f18-da24-4561-a65a-30113cdba96c', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# References\\n\\n1. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n2. Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\\n3. Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n4. Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\\n5. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n6. Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\\n7. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\\n8. Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\\n9. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\\n10. Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\\n11. Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n12. Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\\n13. Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\\n14. Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n15. Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\\n16. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n17. Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\\n18. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n19. Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n20. Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='67f4d246-b7c0-4ad6-b8ec-3e0ec1ec109e', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# References\\n\\n1. Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993.\\n2. David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\\n3. Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\\n4. Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\\n5. Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\\n6. Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\\n7. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\\n8. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n9. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\\n10. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\n11. Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n12. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n13. Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\\n14. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n15. Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n16. Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ee0bb680-ed23-49f5-bbf3-afea527d7207', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Input-Input Layer 5\\n\\n# Attention Visualizations\\n\\nIt is in this spirit that a majority of American governments have passed new laws since 2009 making registration or voting more difficult.\\n\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\\n\\n13', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='db31d91c-f91d-47d8-9cf1-30576b590aa0', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The Law will never be perfect but its application should be just. This is what we are missing, in my opinion.\\n\\nThe Law will never be perfect but its application should be just. This is what we are missing, in my opinion.\\n\\n# Figure 4:\\n\\nTwo attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n14', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a04d4da1-1051-40a2-9ee0-a2e746a1353c', embedding=None, metadata={'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input-Input Layer 5\\n\\napplication perfect should missing opinion\\n\\nThe Law will never be but its be just this is what we are my opinion.\\n\\nInput-Input Layer 5 be The Law will never be perfect but its, application just-should this is what we are missing, in my opinion.\\n\\napplication perfect should missing opinion\\n\\nThe Law will never be but its be just this is what we are my opinion.\\n\\nThe Law will never be perfect but its, application just-should be this is what we are missing, in my opinion.\\n\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\\n\\n15', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    }
   ],
   "source": [
    "# llama parser using llamacloud api to extract content from pdf\n",
    "# 1000 pages free usage per day\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "# bring in our LLAMA_CLOUD_API_KEY\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()\n",
    "\n",
    "# set up parser\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    ")\n",
    "\n",
    "# AW NOTE : markdown 更好，可以使用###来分割不同的段落，可以帮助LLM更好的理解段落信息。使用llama_parse可以将表格，图标，或者代码快转换为markdown格式，并且会提供段落的metadata信息。llama_parse实际上使用后了LLM来解析文档，所以可以提供更好的解析效果。\n",
    "# AW NOTE : PDF的文件质量对解析效果有很大的影响，如果PDF的质量不好，结构不清晰，解析效果可能会很差。\n",
    "\n",
    "\n",
    "# use SimpleDirectoryReader to parse our file\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(input_files=['attention.pdf'], file_extractor=file_extractor).load_data()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
      "\n",
      "# Attention Is All You Need\n",
      "\n",
      "arXiv:1706.03762v7 · [cs.CL] · 2 Aug 2023\n",
      "\n",
      "Ashish Vaswani∗, Noam Shazeer∗, Niki Parmar∗, Jakob Uszkoreit∗\n",
      "\n",
      "Google Brain, Google Brain, Google Research, Google Research\n",
      "\n",
      "avaswani@google.com, noam@google.com, nikip@google.com, usz@google.com\n",
      "\n",
      "Llion Jones∗, Aidan N. Gomez∗ †, Łukasz Kaiser∗\n",
      "\n",
      "Google Research, University of Toronto, Google Brain\n",
      "\n",
      "llion@google.com, aidan@cs.toronto.edu, lukaszkaiser@google.com\n",
      "\n",
      "Illia Polosukhin∗ ‡\n",
      "\n",
      "illia.polosukhin@gmail.com\n",
      "\n",
      "# Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
      "\n",
      "†Work performed while at Google Brain.\n",
      "\n",
      "‡Work performed while at Google Research.\n",
      "\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 1\n",
      "# 1 Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
      "\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "# 2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\n",
      "\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "\n",
      "# 3 Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 2\n",
      "# Figure 1: The Transformer - model architecture.\n",
      "\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n",
      "\n",
      "# 3.1 Encoder and Decoder Stacks\n",
      "\n",
      "# Encoder:\n",
      "\n",
      "The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
      "\n",
      "# Decoder:\n",
      "\n",
      "The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "\n",
      "# 3.2 Attention\n",
      "\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum.\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 3\n",
      "# Scaled Dot-Product Attention\n",
      "\n",
      "# Multi-Head Attention\n",
      "\n",
      "| |Linear|Concat|\n",
      "|---|---|---|\n",
      "|MatMul| |Scaled Dot-Product Attention|\n",
      "|SoftMax|Mask (opt)| |\n",
      "|Scale|MatMul|Linear|\n",
      "| |Linear|Linear|\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "# 3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values.\n",
      "\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:\n",
      "\n",
      "Attention(Q, K, V) = softmax(√dkQKT)V (1)\n",
      "\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi- multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of √1/dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4. To counteract this effect, we scale the dot products by √1/dk.\n",
      "\n",
      "# 3.2.2 Multi-Head Attention\n",
      "\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional variables with mean 0 and variance 1. Then their dot product, q · k = ∑i=1dk qiki, has mean 0 and variance dk.\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 4\n",
      "output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "MultiHead(Q, K, V ) = Concat(head1, ..., headh)WO\n",
      "\n",
      "where headi = Attention(QWiQ, KWiK, VWiV)\n",
      "\n",
      "Where the projections are parameter matrices Wi ∈ Rdmodel×dk, Wi ∈ Rdmodel×dk, Wi ∈ Rdmodel×dv and WO ∈ Rhdv×dmodel.\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "# 3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
      "- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n",
      "\n",
      "# 3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
      "\n",
      "FFN(x) = max(0, xW1 + b1)W2 + b2\n",
      "\n",
      "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.\n",
      "\n",
      "# 3.4 Embeddings and Softmax\n",
      "\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 5\n",
      "# Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "\n",
      "|Layer Type|Complexity per Layer|Sequential Operations|Maximum Path Length|\n",
      "|---|---|---|---|\n",
      "|Self-Attention|O(n2 · d)|O(1)|O(1)|\n",
      "|Recurrent|O(k · n · d)|O(n)|O(logk(n))|\n",
      "|Convolutional|O(r · n · d)|O(1)|O(n/r)|\n",
      "\n",
      "# 3.5 Positional Encoding\n",
      "\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "PE(pos, 2i) = sin(pos/100002i/dmodel)\n",
      "\n",
      "PE(pos, 2i+1) = cos(pos/100002i/dmodel)\n",
      "\n",
      "where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos).\n",
      "\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\n",
      "# 4 Why Self-Attention\n",
      "\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n",
      "\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
      "\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 6\n",
      "length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "\n",
      "A single convolutional layer with kernel width k &lt; n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n",
      "\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
      "\n",
      "# 5 Training\n",
      "\n",
      "This section describes the training regime for our models.\n",
      "\n",
      "# 5.1 Training Data and Batching\n",
      "\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "\n",
      "# 5.2 Hardware and Schedule\n",
      "\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, (described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n",
      "\n",
      "# 5.3 Optimizer\n",
      "\n",
      "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning rate over the course of training, according to the formula:\n",
      "\n",
      "lrate = dmodel−0.5· min(step_num−0.5, step_num · warmup_steps−1.5)\n",
      "\n",
      "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n",
      "\n",
      "# 5.4 Regularization\n",
      "\n",
      "We employ three types of regularization during training:\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 7\n",
      "**Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.**\n",
      "|Model|EN-DE|EN-FR|Training Cost (FLOPs)| |\n",
      "|---|---|---|---|---|\n",
      "|ByteNet [18]|23.75| | | |\n",
      "|Deep-Att + PosUnk [39]| |39.2|1.0 · 1020| |\n",
      "|GNMT + RL [38]|24.6|39.92|2.3 · 1018|1.4 · 1020|\n",
      "|ConvS2S [9]|25.16|40.46|9.6 · 1019|1.5 · 1020|\n",
      "|MoE [32]|26.03|40.56|2.0 · 10|1.2 · 10|\n",
      "|Deep-Att + PosUnk Ensemble [39]| |40.4|8.0 · 1021| |\n",
      "|GNMT + RL Ensemble [38]|26.30|41.16|1.8 · 1019|1.1 · 1021|\n",
      "|ConvS2S Ensemble [9]|26.36|41.29|7.7 · 10|1.2 · 10|\n",
      "|Transformer (base model)|27.3|38.1|3.3 · 1018| |\n",
      "|Transformer (big)|28.4|41.8|2.3 · 10| |\n",
      "\n",
      "Residual Dropout\n",
      "\n",
      "We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\n",
      "\n",
      "Label Smoothing\n",
      "\n",
      "During training, we employed label smoothing of value ϵls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "\n",
      "# 6 Results\n",
      "\n",
      "# 6.1 Machine Translation\n",
      "\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\n",
      "\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU5.\n",
      "\n",
      "# 6.2 Model Variations\n",
      "\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n",
      "\n",
      "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 8\n",
      "# Table 3: Variations on the Transformer architecture.\n",
      "\n",
      "Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
      "\n",
      "|N|dmodel|dff|h|dk|dv|Pdrop|ϵls|train steps|PPL (dev)|BLEU (dev)|params ×106| |\n",
      "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
      "|base|6|512|2048|8|64|64|0.1|0.1|100K|4.92|25.8|65|\n",
      "|(A)|1|512|512| | | | | |5.29|24.9| | |\n",
      "| |4|128|128| | | | | |5.00|25.5| | |\n",
      "| |16|32|32| | | | | |4.91|25.8| | |\n",
      "| |32|16|16| | | | | |5.01|25.4| | |\n",
      "|(B)| |16| | | | | | |5.16|25.1|58| |\n",
      "| | |32| | | | | | |5.01|25.4|60| |\n",
      "| |2| | | | | | | |6.11|23.7|36| |\n",
      "| |4| | | | | | | |5.19|25.3|50| |\n",
      "| |8| | | | | | | |4.88|25.5|80| |\n",
      "|(C)|256| |32|32| | | | |5.75|24.5|28| |\n",
      "| |1024| |128|128| | | | |4.66|26.0|168| |\n",
      "| | |1024| | | | | | |5.12|25.4|53| |\n",
      "| | |4096| | | | | | |4.75|26.2|90| |\n",
      "|(D)| | | | | |0.0| | |5.77|24.6| | |\n",
      "| | | | | | |0.2| | |4.95|25.5| | |\n",
      "| | | | | | |0.0| | |4.67|25.3| | |\n",
      "| | | | | | |0.2| | |5.47|25.7| | |\n",
      "|(E)| | | | | |positional embedding instead of sinusoids| | |4.92|25.7| | |\n",
      "|big|6|1024|4096|16| |0.3| |300K|4.33|26.4|213| |\n",
      "\n",
      "We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
      "\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n",
      "\n",
      "# 6.3 English Constituency Parsing\n",
      "\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 9\n",
      "# Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n",
      "\n",
      "|Parser|Training|WSJ 23 F1|\n",
      "|---|---|---|\n",
      "|Vinyals & Kaiser el al. (2014) [37]|WSJ only, discriminative|88.3|\n",
      "|Petrov et al. (2006) [29]|WSJ only, discriminative|90.4|\n",
      "|Zhu et al. (2013) [40]|WSJ only, discriminative|90.4|\n",
      "|Dyer et al. (2016) [8]|WSJ only, discriminative|91.7|\n",
      "|Transformer (4 layers)|WSJ only, discriminative|91.3|\n",
      "|Zhu et al. (2013) [40]|semi-supervised|91.3|\n",
      "|Huang & Harper (2009) [14]|semi-supervised|91.3|\n",
      "|McClosky et al. (2006) [26]|semi-supervised|92.1|\n",
      "|Vinyals & Kaiser el al. (2014) [37]|semi-supervised|92.1|\n",
      "|Transformer (4 layers)|semi-supervised|92.7|\n",
      "|Luong et al. (2015) [23]|multi-task|93.0|\n",
      "|Dyer et al. (2016) [8]|generative|93.3|\n",
      "\n",
      "increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting.\n",
      "\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
      "\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "\n",
      "# 7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
      "\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "\n",
      "The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\n",
      "\n",
      "# Acknowledgements\n",
      "\n",
      "We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n",
      "\n",
      "# References\n",
      "\n",
      "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n",
      "\n",
      "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      "\n",
      "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      "\n",
      "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 10\n",
      "# References\n",
      "\n",
      "1. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\n",
      "2. Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\n",
      "3. Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n",
      "4. Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\n",
      "5. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n",
      "6. Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\n",
      "7. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\n",
      "8. Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "9. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n",
      "10. Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\n",
      "11. Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n",
      "12. Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\n",
      "13. Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\n",
      "14. Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\n",
      "15. Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\n",
      "16. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "17. Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\n",
      "18. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\n",
      "19. Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n",
      "20. Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 11\n",
      "# References\n",
      "\n",
      "1. Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993.\n",
      "2. David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\n",
      "3. Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\n",
      "4. Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
      "5. Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\n",
      "6. Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\n",
      "7. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\n",
      "8. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\n",
      "9. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\n",
      "10. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\n",
      "11. Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n",
      "12. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
      "13. Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\n",
      "14. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n",
      "15. Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n",
      "16. Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 12\n",
      "# Input-Input Layer 5\n",
      "\n",
      "# Attention Visualizations\n",
      "\n",
      "It is in this spirit that a majority of American governments have passed new laws since 2009 making registration or voting more difficult.\n",
      "\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "\n",
      "13\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 13\n",
      "The Law will never be perfect but its application should be just. This is what we are missing, in my opinion.\n",
      "\n",
      "The Law will never be perfect but its application should be just. This is what we are missing, in my opinion.\n",
      "\n",
      "# Figure 4:\n",
      "\n",
      "Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n",
      "\n",
      "14\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 14\n",
      "Input-Input Layer 5\n",
      "\n",
      "application perfect should missing opinion\n",
      "\n",
      "The Law will never be but its be just this is what we are my opinion.\n",
      "\n",
      "Input-Input Layer 5 be The Law will never be perfect but its, application just-should this is what we are missing, in my opinion.\n",
      "\n",
      "application perfect should missing opinion\n",
      "\n",
      "The Law will never be but its be just this is what we are my opinion.\n",
      "\n",
      "The Law will never be perfect but its, application just-should be this is what we are missing, in my opinion.\n",
      "\n",
      "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "\n",
      "15\n",
      "----\n",
      "{'file_path': 'attention.pdf', 'file_name': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-19', 'last_modified_date': '2024-12-19'}\n",
      "----\n",
      "page number: 15\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(documents)):\n",
    "    print(documents[i].text)\n",
    "    print(\"----\")\n",
    "    print(documents[i].metadata)\n",
    "    print(\"----\")\n",
    "    print(f\"page number: {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout model datalab-to/surya_layout0 on device cuda with dtype torch.float16\n",
      "Loaded texify model to cuda with torch.float16 dtype\n",
      "Loaded recognition model vikp/surya_rec2 on device cuda with dtype torch.float16\n",
      "Loaded table recognition model vikp/surya_tablerec on device cuda with dtype torch.float16\n",
      "Loaded detection model vikp/surya_det3 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recognizing layout: 100%|██████████| 3/3 [00:03<00:00,  1.14s/it]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n",
      "Recognizing equations: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "Recognizing tables: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# marker downloads llm and uses it to extract content from pdf\n",
    "# it is free to use for non-commercial purpose\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.renderers.markdown import MarkdownOutput\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "from marker.config.parser import ConfigParser\n",
    "\n",
    "config = {\n",
    "    \"output_format\":\"markdown\",\n",
    "}\n",
    "config_parser = ConfigParser(config)\n",
    "\n",
    "converter = PdfConverter(\n",
    "    config= config_parser.generate_config_dict(),\n",
    "    artifact_dict = create_model_dict(),\n",
    "    processor_list = config_parser.get_processors(),\n",
    "    renderer = config_parser.get_renderer()\n",
    ")\n",
    "\n",
    "rendered = converter(\"attention.pdf\")\n",
    "text,_,images = text_from_rendered(rendered)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "AW NOTE:\n",
    "- 也是使用LLM进行parse，但是这个使用了本地模型，所以不需要API KEY，但是需要下载LLM模型。\n",
    "- 无法对图片进行解析\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
      "\n",
      "# Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani∗ Google Brain avaswani@google.com\n",
      "\n",
      "> Llion Jones∗ Google Research\n",
      "\n",
      "llion@google.com\n",
      "\n",
      "Google Brain noam@google.com\n",
      "\n",
      "Noam Shazeer∗\n",
      "\n",
      "Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu\n",
      "\n",
      "Niki Parmar∗ Google Research nikip@google.com\n",
      "\n",
      "> Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com\n",
      "\n",
      "Jakob Uszkoreit∗ Google Research usz@google.com\n",
      "\n",
      "Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com\n",
      "\n",
      "# Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "\n",
      "<sup>∗</sup>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
      "\n",
      "<sup>†</sup>Work performed while at Google Brain.\n",
      "\n",
      "<sup>‡</sup>Work performed while at Google Research.\n",
      "\n",
      "# 1 Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
      "\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "# 2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\n",
      "\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "\n",
      "# 3 Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
      "\n",
      "![](_page_2_Figure_0.jpeg)\n",
      "\n",
      "Figure 1: The Transformer - model architecture.\n",
      "\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n",
      "\n",
      "#### 3.1 Encoder and Decoder Stacks\n",
      "\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n",
      "\n",
      "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "\n",
      "### 3.2 Attention\n",
      "\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "\n",
      "Scaled Dot-Product Attention Multi-Head Attention\n",
      "\n",
      "![](_page_3_Figure_1.jpeg)\n",
      "\n",
      "![](_page_3_Figure_2.jpeg)\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "#### 3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by √ dk, and apply a softmax function to obtain the weights on the values.\n",
      "\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n",
      "\n",
      "Attention($Q,K,V$) = softmax($\\frac{QK^{T}}{\\sqrt{d_{k}}}$)$V$ (1)\n",
      "\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of √ 1 dk . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4 . To counteract this effect, we scale the dot products by √ 1 dk .\n",
      "\n",
      "#### 3.2.2 Multi-Head Attention\n",
      "\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "\n",
      "<sup>4</sup>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q · k = Pdk i=1 qiki, has mean 0 and variance dk.\n",
      "\n",
      "output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "$$\\begin{array}{c}{{\\mathrm{MultiHead}(Q,K,V)=\\mathrm{Concat}(\\mathrm{head_{1}},...,\\mathrm{head_{h}})W^{O}}}\\\\ {{\\mathrm{where~head_{i}=Attention}(Q W_{i}^{Q},K W_{i}^{K},V W_{i}^{V})}}\\end{array}$$\n",
      "\n",
      "Where the projections are parameter matrices W Q i ∈ R dmodel×dk , W K i ∈ R dmodel×dk , WV i ∈ R dmodel×dv and WO ∈ R hdv×dmodel .\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "#### 3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "- In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
      "- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n",
      "\n",
      "#### 3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
      "\n",
      "$${\\rm FFN}(x)=\\max(0,xW_{1}+b_{1})W_{2}+b_{2}\\tag{2}$$\n",
      "\n",
      "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality df f = 2048.\n",
      "\n",
      "#### 3.4 Embeddings and Softmax\n",
      "\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel. Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "\n",
      "| Layer Type | Complexity per Layer |  | Sequential Operations | Maximum Path Length |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| Self-Attention | 2 O(n · d) |  | O(1) | O(1) |\n",
      "| Recurrent | 2 O(n · d | ) | O(n) | O(n) |\n",
      "| Convolutional | O(k · n · d | 2 ) | O(1) | O(logk(n)) |\n",
      "| Self-Attention (restricted) | O(r · n · d) |  | O(1) | O(n/r) |\n",
      "\n",
      "#### 3.5 Positional Encoding\n",
      "\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
      "\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "\n",
      "$$\\begin{array}{c}{{P E_{(p o s,2i)}=s i n(p o s/10000^{2i/d_{\\mathrm{model}}})}}\\\\ {{P E_{(p o s,2i+1)}=c o s(p o s/10000^{2i/d_{\\mathrm{model}}})}}\\end{array}$$\n",
      "\n",
      "where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of P Epos.\n",
      "\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "\n",
      "# 4 Why Self-Attention\n",
      "\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi , zi ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n",
      "\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
      "\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
      "\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "\n",
      "length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "\n",
      "A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d 2 ). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n",
      "\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
      "\n",
      "# 5 Training\n",
      "\n",
      "This section describes the training regime for our models.\n",
      "\n",
      "### 5.1 Training Data and Batching\n",
      "\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
      "\n",
      "### 5.2 Hardware and Schedule\n",
      "\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n",
      "\n",
      "### 5.3 Optimizer\n",
      "\n",
      "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9 . We varied the learning rate over the course of training, according to the formula:\n",
      "\n",
      "$$lrate=d_{\\text{model}}^{-0.5}\\cdot\\min(step\\_num^{-0.5},step\\_num\\cdot warmup\\_steps^{-1.5})\\tag{3}$$\n",
      "\n",
      "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n",
      "\n",
      "### 5.4 Regularization\n",
      "\n",
      "We employ three types of regularization during training:\n",
      "\n",
      "| Model | BLEU |  | Training Cost (FLOPs) |  |\n",
      "| --- | --- | --- | --- | --- |\n",
      "|  | EN-DE | EN-FR | EN-DE | EN-FR |\n",
      "| ByteNet [18] | 23.75 |  |  |  |\n",
      "| Deep-Att + PosUnk [39] |  | 39.2 |  | 1.0 · 1020 |\n",
      "| GNMT + RL [38] | 24.6 | 39.92 | 2.3 · 1019 | 1.4 · 1020 |\n",
      "| ConvS2S [9] | 25.16 | 40.46 | 9.6 · 1018 | 1.5 · 1020 |\n",
      "| MoE [32] | 26.03 | 40.56 | 2.0 · 1019 | 1.2 · 1020 |\n",
      "| Deep-Att + PosUnk Ensemble [39] |  | 40.4 |  | 8.0 · 1020 |\n",
      "| GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8 · 1020 | 1.1 · 1021 |\n",
      "| ConvS2S Ensemble [9] | 26.36 | 41.29 | 7.7 · 1019 | 1.2 · 1021 |\n",
      "| Transformer (base model) | 27.3 | 38.1 | 3.3 · 1018 |  |\n",
      "| Transformer (big) | 28.4 | 41.8 | 2.3 · 1019 |  |\n",
      "\n",
      "| Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the |\n",
      "| --- |\n",
      "| English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. |\n",
      "\n",
      "Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\n",
      "\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "\n",
      "# 6 Results\n",
      "\n",
      "#### 6.1 Machine Translation\n",
      "\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\n",
      "\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5 .\n",
      "\n",
      "### 6.2 Model Variations\n",
      "\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n",
      "\n",
      "<sup>5</sup>We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
      "\n",
      "|  | N | dmodel | dff | h | dk | dv | Pdrop | ϵls | train | PPL | BLEU | params ×106 |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "|  |  |  |  |  |  |  |  |  | steps | (dev) | (dev) |  |\n",
      "| base | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65 |\n",
      "| (A) |  |  |  | 1 | 512 | 512 |  |  |  | 5.29 | 24.9 |  |\n",
      "|  |  |  |  | 4 | 128 | 128 |  |  |  | 5.00 | 25.5 |  |\n",
      "|  |  |  |  | 16 | 32 | 32 |  |  |  | 4.91 | 25.8 |  |\n",
      "|  |  |  |  | 32 | 16 | 16 |  |  |  | 5.01 | 25.4 |  |\n",
      "| (B) |  |  |  |  | 16 |  |  |  |  | 5.16 | 25.1 | 58 |\n",
      "|  |  |  |  |  | 32 |  |  |  |  | 5.01 | 25.4 | 60 |\n",
      "|  | 2 8 |  |  |  |  |  |  |  |  | 6.11 | 23.7 | 36 |\n",
      "|  | 4 |  |  |  |  |  |  |  |  | 5.19 | 25.3 | 50 |\n",
      "|  |  |  |  |  |  |  |  |  |  | 4.88 | 25.5 | 80 |\n",
      "| (C) |  | 256 |  |  | 32 | 32 |  |  |  | 5.75 | 24.5 | 28 |\n",
      "|  |  | 1024 |  |  | 128 | 128 |  |  |  | 4.66 | 26.0 | 168 |\n",
      "|  |  |  | 1024 |  |  |  |  |  |  | 5.12 | 25.4 | 53 |\n",
      "|  |  |  | 4096 |  |  |  |  |  |  | 4.75 | 26.2 | 90 |\n",
      "| (D) |  |  |  |  |  |  | 0.0 |  |  | 5.77 4.67 | 24.6 25.3 |  |\n",
      "|  |  |  |  |  |  |  | 0.2 |  |  | 4.95 | 25.5 |  |\n",
      "|  |  |  |  |  |  |  |  | 0.0 |  |  |  |  |\n",
      "|  |  |  |  |  |  |  |  | 0.2 |  | 5.47 | 25.7 |  |\n",
      "| (E) |  |  |  |  | positional embedding instead of sinusoids |  |  |  |  | 4.92 | 25.7 |  |\n",
      "| big | 6 | 1024 | 4096 | 16 |  |  | 0.3 |  | 300K | 4.33 | 26.4 | 213 |\n",
      "\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
      "\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n",
      "\n",
      "### 6.3 English Constituency Parsing\n",
      "\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\n",
      "\n",
      "| Parser | Training | WSJ 23 F1 |\n",
      "| --- | --- | --- |\n",
      "| Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3 |\n",
      "| Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4 |\n",
      "| Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4 |\n",
      "| Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7 |\n",
      "| Transformer (4 layers) | WSJ only, discriminative | 91.3 |\n",
      "| Zhu et al. (2013) [40] | semi-supervised | 91.3 |\n",
      "| Huang & Harper (2009) [14] | semi-supervised | 91.3 |\n",
      "| McClosky et al. (2006) [26] | semi-supervised | 92.1 |\n",
      "| Vinyals & Kaiser el al. (2014) [37] | semi-supervised | 92.1 |\n",
      "| Transformer (4 layers) | semi-supervised | 92.7 |\n",
      "| Luong et al. (2015) [23] | multi-task | 93.0 |\n",
      "| Dyer et al. (2016) [8] | generative | 93.3 |\n",
      "\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n",
      "\n",
      "increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting.\n",
      "\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
      "\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "\n",
      "# 7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
      "\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "\n",
      "The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\n",
      "\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n",
      "\n",
      "# References\n",
      "\n",
      "- [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. *arXiv preprint arXiv:1607.06450*, 2016.\n",
      "- [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. *CoRR*, abs/1409.0473, 2014.\n",
      "- [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. *CoRR*, abs/1703.03906, 2017.\n",
      "- [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. *arXiv preprint arXiv:1601.06733*, 2016.\n",
      "- [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. *CoRR*, abs/1406.1078, 2014.\n",
      "- [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. *arXiv preprint arXiv:1610.02357*, 2016.\n",
      "- [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. *CoRR*, abs/1412.3555, 2014.\n",
      "- [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In *Proc. of NAACL*, 2016.\n",
      "- [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. *arXiv preprint arXiv:1705.03122v2*, 2017.\n",
      "- [10] Alex Graves. Generating sequences with recurrent neural networks. *arXiv preprint arXiv:1308.0850*, 2013.\n",
      "- [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 770–778, 2016.\n",
      "- [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "- [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. *Neural computation*, 9(8):1735–1780, 1997.\n",
      "- [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In *Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing*, pages 832–841. ACL, August 2009.\n",
      "- [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. *arXiv preprint arXiv:1602.02410*, 2016.\n",
      "- [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In *Advances in Neural Information Processing Systems, (NIPS)*, 2016.\n",
      "- [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In *International Conference on Learning Representations (ICLR)*, 2016.\n",
      "- [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. *arXiv preprint arXiv:1610.10099v2*, 2017.\n",
      "- [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In *International Conference on Learning Representations*, 2017.\n",
      "- [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In *ICLR*, 2015.\n",
      "- [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. *arXiv preprint arXiv:1703.10722*, 2017.\n",
      "- [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. *arXiv preprint arXiv:1703.03130*, 2017.\n",
      "- [23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. *arXiv preprint arXiv:1511.06114*, 2015.\n",
      "- [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. *arXiv preprint arXiv:1508.04025*, 2015.\n",
      "- [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. *Computational linguistics*, 19(2):313–330, 1993.\n",
      "- [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In *Proceedings of the Human Language Technology Conference of the NAACL, Main Conference*, pages 152–159. ACL, June 2006.\n",
      "- [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In *Empirical Methods in Natural Language Processing*, 2016.\n",
      "- [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. *arXiv preprint arXiv:1705.04304*, 2017.\n",
      "- [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In *Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL*, pages 433–440. ACL, July 2006.\n",
      "- [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. *arXiv preprint arXiv:1608.05859*, 2016.\n",
      "- [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. *arXiv preprint arXiv:1508.07909*, 2015.\n",
      "- [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. *arXiv preprint arXiv:1701.06538*, 2017.\n",
      "- [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. *Journal of Machine Learning Research*, 15(1):1929–1958, 2014.\n",
      "- [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, *Advances in Neural Information Processing Systems 28*, pages 2440–2448. Curran Associates, Inc., 2015.\n",
      "- [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In *Advances in Neural Information Processing Systems*, pages 3104–3112, 2014.\n",
      "- [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. *CoRR*, abs/1512.00567, 2015.\n",
      "- [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In *Advances in Neural Information Processing Systems*, 2015.\n",
      "- [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. *arXiv preprint arXiv:1609.08144*, 2016.\n",
      "- [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. *CoRR*, abs/1606.04199, 2016.\n",
      "- [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In *Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers)*, pages 434–443. ACL, August 2013.\n",
      "\n",
      "#### Attention Visualizations **Input-Input Layer5**\n",
      "\n",
      "![](_page_12_Figure_1.jpeg)\n",
      "\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color.\n",
      "\n",
      "![](_page_13_Figure_0.jpeg)\n",
      "\n",
      "**Input-Input Layer5**\n",
      "\n",
      "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n",
      "\n",
      "![](_page_14_Figure_0.jpeg)\n",
      "\n",
      "**Input-Input Layer5**\n",
      "\n",
      "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rendered.metadata['table_of_contents'][0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['October 1 - 31, 2023', '', '287,360', '', '$', '404.62', '', '287,360', '', '$', '10,738,584'], ['November 1 - 30, 2023', '', '2,708,477', '', '$', '447.03', '', '2,708,477', '', '$', '9,527,821'], ['December 1 - 31, 2023', '', '2,481,771', '', '$', '472.63', '', '2,481,771', '', '$', '8,354,857'], ['Total', '', '5,477,608', '', '', None, '', '5,477,608', '', '', None]]\n",
      "----\n",
      "Table of Contents\n",
      "PART II\n",
      " \n",
      "Item 5. Market for Registrant’s Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "Market Information\n",
      "Our common stock is traded on the NASDAQ Global Select Market under the symbol “NFLX”.\n",
      "Holders\n",
      "As of December 31, 2023, there were approximately 2,728 stockholders of record of our common stock, although there is a significantly larger number\n",
      "of beneficial owners of our common stock.\n",
      "Dividend Policy\n",
      "We have never declared or paid any cash dividends on our capital stock, and we do not currently anticipate paying any cash dividends in the foreseeable\n",
      "future.\n",
      "Company Purchases of Equity Securities\n",
      "Stock repurchases during the three months ended December 31, 2023 were as follows:\n",
      "Total Number of Shares Approximate Dollar Value\n",
      "Purchased as Part of of Shares that May Yet Be\n",
      "Total Number of Shares Average Price Paid per Publicly Announced Purchased Under the\n",
      "Period Purchased (1) Share (2) Programs (1) Program (1)\n",
      "(in thousands)\n",
      "October 1 - 31, 2023 287,360  $ 404.62  287,360  $ 10,738,584 \n",
      "November 1 - 30, 2023 2,708,477  $ 447.03  2,708,477  $ 9,527,821 \n",
      "December 1 - 31, 2023 2,481,771  $ 472.63  2,481,771  $ 8,354,857 \n",
      "Total 5,477,608  5,477,608 \n",
      "(1) In March 2021, the Company’s Board of Directors authorized the repurchase of up to $5 billion of its common stock, with no expiration date, and in September 2023, the\n",
      "Board of Directors increased the share repurchase authorization by an additional $10 billion, also with no expiration date. For further information regarding stock repurchase\n",
      "activity, see Note 9 Stockholders’ Equity to the consolidated financial statements in this Annual Report.\n",
      "(2) Average price paid per share includes costs associated with the repurchases.\n",
      "19\n",
      "[['Financial Results:', '', '', None, None, '', '', None, None, '', '', None, None, '', '', None], ['Streaming revenues', '', '$', '33,640,458', '', '', '$', '31,469,852', '', '', '$', '29,515,496', '', '', '7', '%'], ['DVD revenues (1)', '', '82,839', None, '', '', '145,698', None, '', '', '182,348', None, '', '', '(43)', '%'], ['Total revenues', '', '$', '33,723,297', '', '', '$', '31,615,550', '', '', '$', '29,697,844', '', '', '7', '%'], ['', '', '', None, None, '', '', None, None, '', '', None, None, '', '', None], ['Operating income', '', '$', '6,954,003', '', '', '$', '5,632,831', '', '', '$', '6,194,509', '', '', '23', '%'], ['Operating margin', '', '21', None, '%', '', '18', None, '%', '', '21', None, '%', '', '', None], ['', '', '', None, None, '', '', None, None, '', '', None, None, '', '', None], ['Global Streaming Memberships:', '', '', None, None, '', '', None, None, '', '', None, None, '', '', None], ['Paid net membership additions', '', '29,529', None, '', '', '8,903', None, '', '', '18,181', None, '', '', '232', '%'], ['Paid memberships at end of period', '', '260,276', None, '', '', '230,747', None, '', '', '221,844', None, '', '', '13', '%'], ['Average paying memberships', '', '240,889', None, '', '', '222,924', None, '', '', '210,784', None, '', '', '8', '%'], ['Average monthly revenue per paying membership', '', '$', '11.64', '', '', '$', '11.76', '', '', '$', '11.67', '', '', '(1)', '%']]\n",
      "----\n",
      "Table of Contents\n",
      "Item 7. Management’s Discussion and Analysis of Financial Condition and Results of Operations\n",
      "This section of this Form 10-K generally discusses 2023 and 2022 items and year-to-year comparisons between 2023 and 2022. Discussions of 2021\n",
      "items and year-to-year comparisons between 2022 and 2021 that are not included in this Form 10-K can be found in \"Management's Discussion and Analysis of\n",
      "Financial Condition and Results of Operations\" in Part II, Item 7 of the Company's Annual Report on Form 10-K for the fiscal year ended December 31, 2022.\n",
      "Results of Operations\n",
      "The following represents our consolidated performance highlights:\n",
      "As of/Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except revenue per membership and percentages)\n",
      "Financial Results:\n",
      "Streaming revenues $ 33,640,458  $ 31,469,852  $ 29,515,496  7 %\n",
      "DVD revenues (1) 82,839  145,698  182,348  (43)%\n",
      "Total revenues $ 33,723,297  $ 31,615,550  $ 29,697,844  7 %\n",
      "Operating income $ 6,954,003  $ 5,632,831  $ 6,194,509  23 %\n",
      "Operating margin 21 % 18 % 21 %\n",
      "Global Streaming Memberships:\n",
      "Paid net membership additions 29,529  8,903  18,181  232 %\n",
      "Paid memberships at end of period 260,276  230,747  221,844  13 %\n",
      "Average paying memberships 240,889  222,924  210,784  8 %\n",
      "Average monthly revenue per paying membership $ 11.64  $ 11.76  $ 11.67  (1)%\n",
      "(1) In April 2023, we announced our plans to discontinue our DVD-by-mail service, and we ceased providing our mailing services to customers on September 29, 2023. The\n",
      "discontinuance of our DVD business had an immaterial impact on our operations and financial results.\n",
      "Consolidated revenues for the year ended December 31, 2023 increased 7% as compared to the year ended December 31, 2022. Operating margin for the\n",
      "year ended December 31, 2023 increased three percentage points, primarily due to revenues growing at a faster rate as compared to the growth in cost of\n",
      "revenues and marketing and decreased technology and development expenses, partially offset by higher growth in general and administrative expenses as\n",
      "compared to the growth in revenues.\n",
      "Streaming Revenues\n",
      "    \n",
      "We primarily derive revenues from monthly membership fees for services related to streaming content to our members. We offer a variety of streaming\n",
      "membership plans, the price of which varies by country and the features of the plan. As of December 31, 2023, pricing on our paid plans ranged from the U.S.\n",
      "dollar equivalent of $1 to $28 per month, and pricing on our extra member sub accounts ranged from the U.S. dollar equivalent of $2 to $8 per month. We\n",
      "expect that from time to time the prices of our membership plans in each country may change and we may test other plan and price variations.\n",
      "We also earn revenue from advertisements presented on our streaming service, consumer products and various other sources. Revenues earned from\n",
      "sources other than monthly membership fees were not material for the years ended December 31, 2023, 2022, and 2021.\n",
      "Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except percentages)\n",
      "Streaming revenues $ 33,640,458  $ 31,469,852  $ 29,515,496  $ 2,170,606  7 %\n",
      "21\n",
      "[['Revenues', '', '$', '14,873,783', '', '$', '14,084,643', '', '$', '12,972,100', '', '$', '789,140', '', '6', '%'], ['Paid net membership additions (losses)', '', '5,832', None, '', '(919)', None, '', '1,279', None, '', '6,751', None, '', '735', '%'], ['Paid memberships at end of period (1)', '', '80,128', None, '', '74,296', None, '', '75,215', None, '', '5,832', None, '', '8', '%'], ['Average paying memberships', '', '76,126', None, '', '74,001', None, '', '74,234', None, '', '2,125', None, '', '3', '%'], ['Average monthly revenue per paying membership', '', '$', '16.28', '', '$', '15.86', '', '$', '14.56', '', '$', '0.42', '', '3', '%'], ['Constant currency change (2)', '', '', None, '', '', None, '', '', None, '', '', None, '', '3', '%']]\n",
      "----\n",
      "Table of Contents\n",
      "Streaming revenues for the year ended December 31, 2023 increased 7% as compared to the year ended December 31, 2022, primarily due to the 8%\n",
      "growth in average paying memberships, partially offset by a 1% decrease in average monthly revenue per paying membership. The decrease in average\n",
      "monthly revenue per paying membership was primarily due to changes in plan mix, higher membership growth in regions with lower average monthly revenue\n",
      "per paying membership, partially offset by limited price increases. Additionally, streaming revenues for the year ended December 31, 2023 were further\n",
      "impacted by unfavorable fluctuations in foreign exchange rates.\n",
      "The following tables summarize streaming revenue and other streaming membership information by region for the years ended December 31, 2023, 2022\n",
      "and 2021.\n",
      "United States and Canada (UCAN)\n",
      "As of/Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except revenue per membership and percentages)\n",
      "Revenues $ 14,873,783  $ 14,084,643  $ 12,972,100  $ 789,140  6 %\n",
      "Paid net membership additions (losses) 5,832  (919) 1,279  6,751  735 %\n",
      "Paid memberships at end of period (1) 80,128  74,296  75,215  5,832  8 %\n",
      "Average paying memberships 76,126  74,001  74,234  2,125  3 %\n",
      "Average monthly revenue per paying membership $ 16.28  $ 15.86  $ 14.56  $ 0.42  3 %\n",
      "Constant currency change (2) 3 %\n",
      "Europe, Middle East, and Africa (EMEA)\n",
      "As of/Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except revenue per membership and percentages)\n",
      "Revenues $ 10,556,487  $ 9,745,015  $ 9,699,819  $ 811,472  8 %\n",
      "Paid net membership additions 12,084  2,693  7,338  9,391  349 %\n",
      "Paid memberships at end of period (1) 88,813  76,729  74,036  12,084  16 %\n",
      "Average paying memberships 80,928  73,904  69,518  7,024  10 %\n",
      "Average monthly revenue per paying membership $ 10.87  $ 10.99  $ 11.63  $ (0.12) (1)%\n",
      "Constant currency change (2) (1)%\n",
      "Latin America (LATAM)\n",
      "As of/Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except revenue per membership and percentages)\n",
      "Revenues $ 4,446,461  $ 4,069,973  $ 3,576,976  $ 376,488  9 %\n",
      "Paid net membership additions 4,298  1,738  2,424  2,560  147 %\n",
      "Paid memberships at end of period (1) 45,997  41,699  39,961  4,298  10 %\n",
      "Average paying memberships 42,802  40,000  38,573  2,802  7 %\n",
      "Average monthly revenue per paying membership $ 8.66  $ 8.48  $ 7.73  $ 0.18  2 %\n",
      "Constant currency change (2) 10 %\n",
      "Asia-Pacific (APAC)\n",
      "22\n",
      "[['Revenues', '', '$', '3,763,727', '', '$', '3,570,221', '$', '3,266,601', '$', '193,506', '5', '%'], ['Paid net membership additions', '', '7,315', None, '', '5,391', None, '7,140', None, '1,924', None, '36', '%'], ['Paid memberships at end of period (1)', '', '45,338', None, '', '38,023', None, '32,632', None, '7,315', None, '19', '%'], ['Average paying memberships', '', '41,033', None, '', '35,019', None, '28,461', None, '6,014', None, '17', '%'], ['Average monthly revenue per paying membership', '', '$', '7.64', '', '$', '8.50', '$', '9.56', '$', '(0.86)', '(10)', '%'], ['Constant currency change (2)', '', '', None, '', '', None, '', None, '', None, '(6)', '%']]\n",
      "----\n",
      "Table of Contents\n",
      "As of/Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except revenue per membership and percentages)\n",
      "Revenues $ 3,763,727  $ 3,570,221  $ 3,266,601  $ 193,506  5 %\n",
      "Paid net membership additions 7,315  5,391  7,140  1,924  36 %\n",
      "Paid memberships at end of period (1) 45,338  38,023  32,632  7,315  19 %\n",
      "Average paying memberships 41,033  35,019  28,461  6,014  17 %\n",
      "Average monthly revenue per paying membership $ 7.64  $ 8.50  $ 9.56  $ (0.86) (10)%\n",
      "Constant currency change (2) (6)%\n",
      "(1) A paid membership (also referred to as a paid subscription) is defined as a membership that has the right to receive Netflix service following sign-up and a method of\n",
      "payment being provided, and that is not part of a free trial or certain other promotions that may be offered by the Company to new or rejoining members. Certain members have\n",
      "the option to add extra member sub accounts. These extra member sub accounts are not included in paid memberships. A membership is canceled and ceases to be reflected in\n",
      "the above metrics as of the effective cancellation date. Voluntary cancellations generally become effective at the end of the prepaid membership period. Involuntary\n",
      "cancellations, as a result of a failed method of payment, become effective immediately. Memberships are assigned to territories based on the geographic location used at time of\n",
      "sign-up as determined by the Company’s internal systems, which utilize industry standard geo-location technology.\n",
      "(2) We believe the non-GAAP financial measure of constant currency revenue is useful in analyzing the underlying trends in average monthly revenue per paying membership\n",
      "absent foreign currency fluctuations. However, this non-GAAP financial measure should be considered in addition to, not as a substitute for, or superior to other financial\n",
      "measures prepared in accordance with GAAP. In order to exclude the effect of foreign currency rate fluctuations on average monthly revenue per paying membership, we\n",
      "estimate current period revenue assuming foreign exchange rates had remained constant with foreign exchange rates from each of the corresponding months of the prior-year\n",
      "period. For the year ended December 31, 2023, our revenues would have been approximately $597 million higher had foreign currency exchange rates remained constant with\n",
      "those for the year ended December 31, 2022.\n",
      "Cost of Revenues\n",
      "Amortization of content assets makes up the majority of cost of revenues. Expenses directly associated with the acquisition, licensing and production of\n",
      "content (such as payroll, stock-based compensation, facilities, and other related personnel expenses, costs associated with obtaining rights to music included in\n",
      "our content, overall deals with talent, miscellaneous production related costs and participations and residuals), streaming delivery costs and other operations\n",
      "costs make up the remainder of cost of revenues. We have built our own global content delivery network (“Open Connect”) to help us efficiently stream a high\n",
      "volume of content to our members over the internet. Delivery expenses, therefore, include equipment costs related to Open Connect, payroll and related\n",
      "personnel expenses and all third-party costs, such as cloud computing costs, associated with delivering content over the internet. Other operations costs include\n",
      "customer service and payment processing fees, including those we pay to our integrated payment partners, as well as other costs directly incurred in making our\n",
      "content available to members.\n",
      "  Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except percentages)\n",
      "Cost of revenues $ 19,715,368  $ 19,168,285  $ 17,332,683  $ 547,083  3 %\n",
      "As a percentage of revenues 58 % 61 % 58 %\n",
      "The increase in cost of revenues for the year ended December 31, 2023 as compared to the year ended December 31, 2022 was due to a $171 million\n",
      "increase in content amortization relating to our existing and new content, coupled with a $376 million increase in other cost of revenues primarily due to an\n",
      "increase in expenses directly associated with the acquisition, licensing and production of content.\n",
      "Marketing\n",
      "Marketing expenses consist primarily of advertising expenses and certain payments made to our marketing and advertising sales partners, including\n",
      "consumer electronics (\"CE\") manufacturers, multichannel video programming distributors (\"MVPDs\"), mobile operators and ISPs. Advertising expenses\n",
      "include promotional activities such as digital and television advertising. Marketing expenses also include payroll, stock-based compensation, facilities, and\n",
      "other related expenses for personnel that support sales and marketing activities.\n",
      "23\n",
      "[['Marketing', '', '$', '2,657,883', '', '$', '2,530,502', '', '$', '2,545,146', '', '$', '127,381', '5', '%']]\n",
      "----\n",
      "Table of Contents\n",
      "  Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except percentages)\n",
      "Marketing $ 2,657,883  $ 2,530,502  $ 2,545,146  $ 127,381  5 %\n",
      "As a percentage of revenues 8 % 8 % 9 %\n",
      "The increase in marketing expenses for the year ended December 31, 2023 as compared to the year ended December 31, 2022 was primarily due to a\n",
      "$146 million increase in advertising expenses and a $21 million increase in personnel-related costs, partially offset by a $39 million decrease in payments to\n",
      "our marketing partners.\n",
      "Technology and Development\n",
      "Technology and development expenses consist primarily of payroll, stock-based compensation, facilities, and other related expenses for technology\n",
      "personnel responsible for making improvements to our service offerings, including testing, maintaining and modifying our user interface, our\n",
      "recommendations, merchandising and infrastructure. Technology and development expenses also include costs associated with general use computer hardware\n",
      "and software.\n",
      " \n",
      "  Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except percentages)\n",
      "Technology and development $ 2,675,758  $ 2,711,041  $ 2,273,885  $ (35,283) (1)%\n",
      "As a percentage of revenues 8 % 9 % 8 %\n",
      "Technology and development expenses for the year ended December 31, 2023 as compared to the year ended December 31, 2022 remained relatively flat.\n",
      "General and Administrative\n",
      "General and administrative expenses consist of payroll, stock-based compensation, facilities, and other related expenses for corporate personnel. General\n",
      "and administrative expenses also include professional fees and other general corporate expenses.\n",
      "  Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except percentages)\n",
      "General and administrative $ 1,720,285  $ 1,572,891  $ 1,351,621  $ 147,394  9 %\n",
      "As a percentage of revenues 5 % 5 % 5 %\n",
      "The increase in general and administrative expenses for the year ended December 31, 2023 as compared to the year ended December 31, 2022 was\n",
      "primarily due to a $82 million increase in third-party expenses and a $78 million increase in personnel-related costs.\n",
      "Interest Expense\n",
      "Interest expense consists primarily of the interest associated with our outstanding debt obligations, including the amortization of debt issuance costs. See\n",
      "Note 6 Debt in the accompanying notes to our consolidated financial statements for further detail on our debt obligations.\n",
      " \n",
      "  Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except percentages)\n",
      "Interest expense $ 699,826  $ 706,212  $ 765,620  $ (6,386) (1)%\n",
      "As a percentage of revenues 2 % 2 % 3 %\n",
      "24\n",
      "[['Interest and other income (expense)', '', '$', '(48,772)', '', '$', '337,310', '', '$', '411,214', '', '$', '(386,082)', '', '(114)', '%']]\n",
      "----\n",
      "Table of Contents\n",
      "Interest expense for the year ended December 31, 2023 consisted primarily of $698 million of interest on our Notes. Interest expense for the year ended\n",
      "December 31, 2023 as compared to the year ended December 31, 2022 remained relatively flat.\n",
      "Interest and Other Income (Expense)\n",
      "Interest and other income (expense) consists primarily of foreign exchange gains and losses on foreign currency denominated balances and interest earned\n",
      "on cash, cash equivalents and short-term investments.\n",
      "  Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except percentages)\n",
      "Interest and other income (expense) $ (48,772) $ 337,310  $ 411,214  $ (386,082) (114)%\n",
      "As a percentage of revenues — % 1 % 1 %\n",
      "Interest and other income (expense) decreased primarily due to foreign exchange losses of $293 million for the year ended December 31, 2023 as\n",
      "compared to a gain of $282 million for the year ended December 31, 2022. The foreign exchange loss in the year ended December 31, 2023 was primarily\n",
      "driven by the non-cash loss of $176 million from the remeasurement of our Senior Notes denominated in euros, coupled with the remeasurement of cash and\n",
      "content liability positions in currencies other than the functional currencies. The foreign exchange gain in the year ended December 31, 2022 was primarily\n",
      "driven by the non-cash $353 million gain from the remeasurement of our Senior Notes denominated in euros, partially offset by the remeasurement of cash and\n",
      "content liability positions in currencies other than the functional currencies. The change in foreign currency gains and losses was partially offset by a\n",
      "$221 million increase in interest income earned due to higher average interest rates and investment balances for the year ended December 31, 2023 as\n",
      "compared to the year ended December 31, 2022.\n",
      "Provision for Income Taxes\n",
      "  Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "  (in thousands, except percentages)\n",
      "Provision for income taxes $ 797,415  $ 772,005  $ 723,875  $ 25,410  3 %\n",
      "Effective tax rate 13 % 15 % 12 %\n",
      "The decrease in our effective tax rate for the year ended December 31, 2023 as compared to the year ended December 31, 2022 is primarily due to a\n",
      "decrease in foreign taxes. See Note 10 Income Taxes to the consolidated financial statements for further information regarding income taxes.\n",
      "25\n",
      "[['Content obligations (1)', '', '$', '21,713,349', '', '$', '10,328,923', '', '$', '11,384,426'], ['Debt (2)', '', '17,739,159', None, '', '1,077,261', None, '', '16,661,898', None], ['Operating lease obligations (3)', '', '3,088,899', None, '', '513,506', None, '', '2,575,393', None], ['Total', '', '$', '42,541,407', '', '$', '11,919,690', '', '$', '30,621,717']]\n",
      "----\n",
      "Table of Contents\n",
      "Liquidity and Capital Resources\n",
      "As of December 31, Change\n",
      "2023 2022 2023 vs. 2022\n",
      "(in thousands, except percentages)\n",
      "Cash, cash equivalents, restricted cash and short-term investments $ 7,139,488  $ 6,081,858  $ 1,057,630  17 %\n",
      "Short-term and long-term debt 14,543,261  14,353,076  190,185  1 %\n",
      "Cash, cash equivalents, restricted cash and short-term investments increased $1,058 million in the year ended December 31, 2023 primarily due to cash\n",
      "provided by operations, partially offset by the repurchase of stock.\n",
      "Debt, net of debt issuance costs, increased $190 million primarily due to the remeasurement of our euro-denominated notes. The amount of principal and\n",
      "interest due in the next twelve months is $1,077 million. The amount of principal and interest due beyond the next twelve months is $16,662 million. As\n",
      "of December 31, 2023, no amounts had been borrowed under our $1 billion Revolving Credit Agreement. See Note 6 Debt in the accompanying notes to our\n",
      "consolidated financial statements.\n",
      "We anticipate that our future capital needs from the debt market will be more limited compared to prior years. Our ability to obtain this or any additional\n",
      "financing that we may choose or need, including for potential strategic acquisitions and investments, will depend on, among other things, our development\n",
      "efforts, business plans, operating performance, and the condition of the capital markets at the time we seek financing. We may not be able to obtain such\n",
      "financing on terms acceptable to us or at all. If we raise additional funds through the issuance of equity or debt securities, those securities may have rights,\n",
      "preferences or privileges senior to the rights of our common stock, and our stockholders may experience dilution.\n",
      "In March 2021, our Board of Directors authorized the repurchase of up to $5 billion of our common stock, with no expiration date, and in September\n",
      "2023, the Board of Directors increased the share repurchase authorization by an additional $10 billion, also with no expiration date. Stock repurchases may be\n",
      "effected through open market repurchases in compliance with Rule 10b-18 under the Exchange Act, including through the use of trading plans intended to\n",
      "qualify under Rule 10b5-1 under the Exchange Act, privately-negotiated transactions, accelerated stock repurchase plans, block purchases, or other similar\n",
      "purchase techniques and in such amounts as management deems appropriate. We are not obligated to repurchase any specific number of shares, and the timing\n",
      "and actual number of shares repurchased will depend on a variety of factors, including our stock price, general economic, business and market conditions, and\n",
      "alternative investment opportunities. We may discontinue any repurchases of our common stock at any time without prior notice. In the fiscal year ended\n",
      "December 31, 2023, the Company repurchased 14,513,790 shares of common stock for an aggregate amount of $6,045 million. As of December 31, 2023,\n",
      "$8.4 billion remains available for repurchases.\n",
      "Our primary uses of cash include the acquisition, licensing and production of content, marketing programs, streaming delivery and personnel-related\n",
      "costs, as well as strategic acquisitions and investments. Cash payment terms for non-original content have historically been in line with the amortization period.\n",
      "Investments in original content, and in particular content that we produce and own, require more cash upfront relative to licensed content. For example,\n",
      "production costs are paid as the content is created, well in advance of when the content is available on the service and amortized. We expect to continue to\n",
      "significantly invest in global content, particularly in original content, which will impact our liquidity. We currently anticipate that cash flows from operations,\n",
      "available funds and access to financing sources, including our revolving credit facility, will continue to be sufficient to meet our cash needs for the next twelve\n",
      "months and beyond.\n",
      "Our material cash requirements from known contractual and other obligations primarily relate to our content, debt and lease obligations. As of December\n",
      "31, 2023, the expected timing of those payments are as follows:\n",
      "Contractual obligations (in thousands): Total Next 12 Months Beyond 12 Months\n",
      "Content obligations (1) $ 21,713,349  $ 10,328,923  $ 11,384,426 \n",
      "Debt (2) 17,739,159  1,077,261  16,661,898 \n",
      "Operating lease obligations (3) 3,088,899  513,506  2,575,393 \n",
      "Total $ 42,541,407  $ 11,919,690  $ 30,621,717 \n",
      " \n",
      "(1) As of December 31, 2023, content obligations were comprised of $4.5 billion included in \"Current content liabilities\" and $2.6 billion of \"Non-current\n",
      "content liabilities\" on the Consolidated Balance Sheets and $14.6 billion of obligations that are not reflected on the Consolidated Balance Sheets as they\n",
      "did not then meet the criteria for recognition.\n",
      "26\n",
      "[['Net cash provided by operating activities', '$', '7,274,301', '', '$', '2,026,257', '', '$', '392,610', '', '$', '5,248,044', '259', '%'], ['Net cash provided by (used in) investing activities', '541,751', None, '', '(2,076,392)', None, '', '(1,339,853)', None, '', '2,618,143', None, '126', '%'], ['Net cash used in financing activities', '(5,950,803)', None, '', '(664,254)', None, '', '(1,149,776)', None, '', '5,286,549', None, '796', '%'], ['', '', None, '', '', None, '', '', None, '', '', None, '', None], ['Non-GAAP reconciliation of free cash flow:', '', None, '', '', None, '', '', None, '', '', None, '', None], ['Net cash provided by operating activities', '7,274,301', None, '', '2,026,257', None, '', '392,610', None, '', '5,248,044', None, '259', '%'], ['Purchases of property and equipment', '(348,552)', None, '', '(407,729)', None, '', '(524,585)', None, '', '(59,177)', None, '(15)', '%'], ['Change in other assets', '—', None, '', '—', None, '', '(26,919)', None, '', '—', None, '—', '%'], ['Free cash flow', '$', '6,925,749', '', '$', '1,618,528', '', '$', '(158,894)', '', '$', '5,307,221', '328', '%']]\n",
      "----\n",
      "Table of Contents\n",
      "Content obligations include amounts related to the acquisition, licensing and production of content. An obligation for the production of content includes\n",
      "non-cancelable commitments under creative talent and employment agreements and other production related commitments. An obligation for the\n",
      "acquisition and licensing of content is incurred at the time we enter into an agreement to obtain future titles. Once a title becomes available, a content\n",
      "liability is recorded on the Consolidated Balance Sheets. Certain agreements include the obligation to license rights for unknown future titles, the\n",
      "ultimate quantity and/or fees for which are not yet determinable as of the reporting date. Traditional film output deals, or certain TV series license\n",
      "agreements where the number of seasons to be aired is unknown, are examples of these types of agreements. The contractual obligations table above\n",
      "does not include any estimated obligation for the unknown future titles, payment for which could range from less than one year to more than five years.\n",
      "However, these unknown obligations are expected to be significant and we believe could include approximately $1 billion to $4 billion over the next\n",
      "three years, with the payments for the vast majority of such amounts expected to occur after the next twelve months. The foregoing range is based on\n",
      "considerable management judgments and the actual amounts may differ. Once we know the title that we will receive and the license fees, we include the\n",
      "amount in the contractual obligations table above.\n",
      "(2) Debt obligations include our Notes consisting of principal and interest payments. See Note 6 Debt in the accompanying notes to our consolidated\n",
      "financial statements for further details.\n",
      "(3) Operating lease obligations are comprised of operating lease liabilities included in \"Accrued expenses and other liabilities\" and \"Other non-current\n",
      "liabilities\" on the Consolidated Balance Sheets, inclusive of imputed interest. Operating lease obligations also include additional obligations that are not\n",
      "reflected on the Consolidated Balance Sheets as they did not meet the criteria for recognition. As of December 31, 2023, the Company has additional\n",
      "operating leases for real estate that have not yet commenced of $343 million which has been included above. See Note 5 Balance Sheet Components in\n",
      "the accompanying notes to our consolidated financial statements for further details regarding leases.\n",
      "In addition, as of December 31, 2023, we had gross unrecognized tax benefits of $327 million, of which $221 million was classified in “Other non-\n",
      "current liabilities\" in the Consolidated Balance Sheets. At this time, an estimate of the range of reasonably possible adjustments to the balance of unrecognized\n",
      "tax benefits cannot be made.\n",
      "Free Cash Flow\n",
      "We define free cash flow as cash provided by (used in) operating activities less purchases of property and equipment and change in other assets. We\n",
      "believe free cash flow is an important liquidity metric because it measures, during a given period, the amount of cash generated that is available to repay debt\n",
      "obligations, make strategic acquisitions and investments and for certain other activities like stock repurchases. Free cash flow is considered a non-GAAP\n",
      "financial measure and should not be considered in isolation of, or as a substitute for, net income, operating income, net cash provided by operating activities, or\n",
      "any other measure of financial performance or liquidity presented in accordance with GAAP.\n",
      "In assessing liquidity in relation to our results of operations, we compare free cash flow to net income, noting that the major recurring differences are the\n",
      "timing impact between content payments and amortization, non-cash stock-based compensation expense, non-cash remeasurement gain/loss on our euro-\n",
      "denominated debt, excess property and equipment purchases over depreciation, and other working capital differences. Working capital differences primarily\n",
      "include deferred revenue, taxes and semi-annual interest payments on our outstanding debt. Our receivables from members generally settle quickly.\n",
      "  Year Ended December 31, Change\n",
      "  2023 2022 2021 2023 vs. 2022\n",
      "(in thousands)\n",
      "Net cash provided by operating activities $ 7,274,301  $ 2,026,257  $ 392,610  $ 5,248,044  259 %\n",
      "Net cash provided by (used in) investing activities 541,751  (2,076,392) (1,339,853) 2,618,143  126 %\n",
      "Net cash used in financing activities (5,950,803) (664,254) (1,149,776) 5,286,549  796 %\n",
      "Non-GAAP reconciliation of free cash flow:\n",
      "Net cash provided by operating activities 7,274,301  2,026,257  392,610  5,248,044  259 %\n",
      "Purchases of property and equipment (348,552) (407,729) (524,585) (59,177) (15)%\n",
      "Change in other assets —  —  (26,919) —  — %\n",
      "Free cash flow $ 6,925,749  $ 1,618,528  $ (158,894) $ 5,307,221  328 %\n",
      "27\n",
      "[['Ted Sarandos (1)', '', 'Co-CEO and Director', '', 'Adoption', '', '11/10/2023', '', '2/7/2025', '', '68,957']]\n",
      "----\n",
      "Table of Contents\n",
      "Item 9B. Other Information\n",
      "Rule 10b5-1 Trading Plans\n",
      "The adoption or termination of contracts, instructions or written plans for the purchase or sale of our securities by our Section 16 officers and directors for\n",
      "the three months ended December 31, 2023, each of which is intended to satisfy the affirmative defense conditions of Rule 10b5-1(c) under the Exchange Act\n",
      "(“Rule 10b5-1 Plan”), were as follows:\n",
      "Aggregate # of Securities to\n",
      "Name Title Action Date Adopted Expiration Date be Purchased/Sold\n",
      "Ted Sarandos (1) Co-CEO and Director Adoption 11/10/2023 2/7/2025 68,957\n",
      "(1) Ted Sarandos, co-CEO and a member of the Board of Directors, entered into a pre-arranged stock trading plan pursuant to Rule 10b5-1 on November 10, 2023. Mr.\n",
      "Sarandos' plan provides for the potential exercise of vested stock options and the associated sale of up to 68,957 shares of Netflix common stock. The plan expires on February\n",
      "7, 2025, or upon the earlier completion of all authorized transactions under the plan.\n",
      "Other than those disclosed above, none of our directors or officers adopted or terminated a \"non-Rule 10b5-1 trading arrangement\" as defined in Item 408 of\n",
      "Regulation S-K.\n",
      "Item 9C. Disclosure Regarding Foreign Jurisdictions that Prevent Inspections\n",
      "Not applicable.\n",
      "33\n",
      "[['Revenues', '', '$', '33,723,297', '', '$', '31,615,550', '$', '29,697,844'], ['Cost of revenues', '', '19,715,368', None, '', '19,168,285', None, '17,332,683', None], ['Marketing', '', '2,657,883', None, '', '2,530,502', None, '2,545,146', None], ['Technology and development', '', '2,675,758', None, '', '2,711,041', None, '2,273,885', None], ['General and administrative', '', '1,720,285', None, '', '1,572,891', None, '1,351,621', None], ['Operating income', '', '6,954,003', None, '', '5,632,831', None, '6,194,509', None], ['Other income (expense):', '', '', None, '', '', None, '', None], ['Interest expense', '', '(699,826)', None, '', '(706,212)', None, '(765,620)', None], ['Interest and other income (expense)', '', '(48,772)', None, '', '337,310', None, '411,214', None], ['Income before income taxes', '', '6,205,405', None, '', '5,263,929', None, '5,840,103', None], ['Provision for income taxes', '', '(797,415)', None, '', '(772,005)', None, '(723,875)', None], ['Net income', '', '$', '5,407,990', '', '$', '4,491,924', '$', '5,116,228'], ['', '', '', None, '', '', None, '', None], ['Earnings per share:', '', '', None, '', '', None, '', None], ['Basic', '', '$', '12.25', '', '$', '10.10', '$', '11.55'], ['Diluted', '', '$', '12.03', '', '$', '9.95', '$', '11.24'], ['Weighted-average shares of common stock outstanding:', '', '', None, '', '', None, '', None], ['Basic', '', '441,571', None, '', '444,698', None, '443,155', None], ['Diluted', '', '449,498', None, '', '451,290', None, '455,372', None]]\n",
      "----\n",
      "Table of Contents\n",
      "NETFLIX, INC.\n",
      "CONSOLIDATED STATEMENTS OF OPERATIONS\n",
      "(in thousands, except per share data)\n",
      " \n",
      "   Year ended December 31,\n",
      "   2023 2022 2021\n",
      "Revenues $ 33,723,297  $ 31,615,550  $ 29,697,844 \n",
      "Cost of revenues 19,715,368  19,168,285  17,332,683 \n",
      "Marketing 2,657,883  2,530,502  2,545,146 \n",
      "Technology and development 2,675,758  2,711,041  2,273,885 \n",
      "General and administrative 1,720,285  1,572,891  1,351,621 \n",
      "Operating income 6,954,003  5,632,831  6,194,509 \n",
      "Other income (expense):\n",
      "Interest expense (699,826) (706,212) (765,620)\n",
      "Interest and other income (expense) (48,772) 337,310  411,214 \n",
      "Income before income taxes 6,205,405  5,263,929  5,840,103 \n",
      "Provision for income taxes (797,415) (772,005) (723,875)\n",
      "Net income $ 5,407,990  $ 4,491,924  $ 5,116,228 \n",
      "Earnings per share:\n",
      "Basic $ 12.25  $ 10.10  $ 11.55 \n",
      "Diluted $ 12.03  $ 9.95  $ 11.24 \n",
      "Weighted-average shares of common stock outstanding:\n",
      "Basic 441,571  444,698  443,155 \n",
      "Diluted 449,498  451,290  455,372 \n",
      "See accompanying notes to consolidated financial statements.\n",
      "39\n",
      "[['', None, None, None, None, None, None, None], ['Net income', '$', '5,407,990', '', '$', '4,491,924', '$', '5,116,228'], ['Other comprehensive income (loss):', '', None, '', '', None, '', None], ['Foreign currency translation adjustments', '113,384', None, '', '(176,811)', None, '(84,893)', None], ['Cash flow hedges:', '', None, '', '', None, '', None], ['Net unrealized gains (losses), net of tax benefit (expense) of $36 million, $0, and $0,\\nrespectively', '(120,023)', None, '', '—', None, '—', None], ['Total other comprehensive loss', '(6,639)', None, '', '(176,811)', None, '(84,893)', None], ['Comprehensive income', '$', '5,401,351', '', '$', '4,315,113', '$', '5,031,335']]\n",
      "----\n",
      "Table of Contents\n",
      "NETFLIX, INC.\n",
      "CONSOLIDATED STATEMENTS OF COMPREHENSIVE INCOME\n",
      "(in thousands)\n",
      "Year ended December 31,\n",
      "2023 2022 2021\n",
      "Net income $ 5,407,990  $ 4,491,924  $ 5,116,228 \n",
      "Other comprehensive income (loss):\n",
      "Foreign currency translation adjustments 113,384  (176,811) (84,893)\n",
      "Cash flow hedges:\n",
      "Net unrealized gains (losses), net of tax benefit (expense) of $36 million, $0, and $0, (120,023) —  — \n",
      "respectively\n",
      "Total other comprehensive loss (6,639) (176,811) (84,893)\n",
      "Comprehensive income $ 5,401,351  $ 4,315,113  $ 5,031,335 \n",
      "See accompanying notes to consolidated financial statements.\n",
      "40\n",
      "[['Cash flows from operating activities:', '', '', None, '', '', None, '', '', None], ['Net income', '', '$', '5,407,990', '', '$', '4,491,924', '', '$', '5,116,228'], ['Adjustments to reconcile net income to net cash provided by operating activities:', '', '', None, '', '', None, '', '', None], ['Additions to content assets', '', '(12,554,703)', None, '', '(16,839,038)', None, '', '(17,702,202)', None], ['Change in content liabilities', '', '(585,602)', None, '', '179,310', None, '', '232,898', None], ['Amortization of content assets', '', '14,197,437', None, '', '14,026,132', None, '', '12,230,367', None], ['Depreciation and amortization of property, equipment and intangibles', '', '356,947', None, '', '336,682', None, '', '208,412', None], ['Stock-based compensation expense', '', '339,368', None, '', '575,452', None, '', '403,220', None], ['Foreign currency remeasurement loss (gain) on debt', '', '176,296', None, '', '(353,111)', None, '', '(430,661)', None], ['Other non-cash items', '', '512,075', None, '', '533,543', None, '', '376,777', None], ['Deferred income taxes', '', '(459,359)', None, '', '(166,550)', None, '', '199,548', None], ['Changes in operating assets and liabilities:', '', '', None, '', '', None, '', '', None], ['Other current assets', '', '(181,003)', None, '', '(353,834)', None, '', '(369,681)', None], ['Accounts payable', '', '93,502', None, '', '(158,543)', None, '', '145,115', None], ['Accrued expenses and other liabilities', '', '103,565', None, '', '(55,513)', None, '', '180,338', None], ['Deferred revenue', '', '178,708', None, '', '27,356', None, '', '91,350', None], ['Other non-current assets and liabilities', '', '(310,920)', None, '', '(217,553)', None, '', '(289,099)', None], ['Net cash provided by operating activities', '', '7,274,301', None, '', '2,026,257', None, '', '392,610', None], ['Cash flows from investing activities:', '', '', None, '', '', None, '', '', None], ['Purchases of property and equipment', '', '(348,552)', None, '', '(407,729)', None, '', '(524,585)', None], ['Change in other assets', '', '—', None, '', '—', None, '', '(26,919)', None], ['Acquisitions', '', '—', None, '', '(757,387)', None, '', '(788,349)', None], ['Purchases of short-term investments', '', '(504,862)', None, '', '(911,276)', None, '', '—', None], ['Proceeds from maturities of short-term investments', '', '1,395,165', None, '', '—', None, '', '—', None], ['Net cash provided by (used in) investing activities', '', '541,751', None, '', '(2,076,392)', None, '', '(1,339,853)', None], ['Cash flows from financing activities:', '', '', None, '', '', None, '', '', None], ['Repayments of debt', '', '—', None, '', '(700,000)', None, '', '(500,000)', None], ['Proceeds from issuance of common stock', '', '169,990', None, '', '35,746', None, '', '174,414', None], ['Repurchases of common stock', '', '(6,045,347)', None, '', '—', None, '', '(600,022)', None], ['Taxes paid related to net share settlement of equity awards', '', '—', None, '', '—', None, '', '(224,168)', None], ['Other financing activities', '', '(75,446)', None, '', '—', None, '', '—', None], ['Net cash used in financing activities', '', '(5,950,803)', None, '', '(664,254)', None, '', '(1,149,776)', None], ['Effect of exchange rate changes on cash, cash equivalents and restricted cash', '', '82,684', None, '', '(170,140)', None, '', '(86,740)', None], ['Net increase (decrease) in cash, cash equivalents and restricted cash', '', '1,947,933', None, '', '(884,529)', None, '', '(2,183,759)', None], ['Cash, cash equivalents and restricted cash, beginning of year', '', '5,170,582', None, '', '6,055,111', None, '', '8,238,870', None], ['Cash, cash equivalents and restricted cash, end of year', '', '$', '7,118,515', '', '$', '5,170,582', '', '$', '6,055,111'], ['Supplemental disclosure:', '', '', None, '', '', None, '', '', None], ['Income taxes paid', '', '$', '1,154,973', '', '$', '811,720', '', '$', '509,265'], ['Interest paid', '', '684,504', None, '', '701,693', None, '', '763,432', None]]\n",
      "----\n",
      "Table of Contents\n",
      "NETFLIX, INC.\n",
      "CONSOLIDATED STATEMENTS OF CASH FLOWS\n",
      "(in thousands)\n",
      "   Year Ended December 31,\n",
      "   2023 2022 2021\n",
      "Cash flows from operating activities:\n",
      "Net income $ 5,407,990  $ 4,491,924  $ 5,116,228 \n",
      "Adjustments to reconcile net income to net cash provided by operating activities:\n",
      "Additions to content assets (12,554,703) (16,839,038) (17,702,202)\n",
      "Change in content liabilities (585,602) 179,310  232,898 \n",
      "Amortization of content assets 14,197,437  14,026,132  12,230,367 \n",
      "Depreciation and amortization of property, equipment and intangibles 356,947  336,682  208,412 \n",
      "Stock-based compensation expense 339,368  575,452  403,220 \n",
      "Foreign currency remeasurement loss (gain) on debt 176,296  (353,111) (430,661)\n",
      "Other non-cash items 512,075  533,543  376,777 \n",
      "Deferred income taxes (459,359) (166,550) 199,548 \n",
      "Changes in operating assets and liabilities:\n",
      "Other current assets (181,003) (353,834) (369,681)\n",
      "Accounts payable 93,502  (158,543) 145,115 \n",
      "Accrued expenses and other liabilities 103,565  (55,513) 180,338 \n",
      "Deferred revenue 178,708  27,356  91,350 \n",
      "Other non-current assets and liabilities (310,920) (217,553) (289,099)\n",
      "Net cash provided by operating activities 7,274,301  2,026,257  392,610 \n",
      "Cash flows from investing activities:\n",
      "Purchases of property and equipment (348,552) (407,729) (524,585)\n",
      "Change in other assets —  —  (26,919)\n",
      "Acquisitions —  (757,387) (788,349)\n",
      "Purchases of short-term investments (504,862) (911,276) — \n",
      "Proceeds from maturities of short-term investments 1,395,165  —  — \n",
      "Net cash provided by (used in) investing activities 541,751  (2,076,392) (1,339,853)\n",
      "Cash flows from financing activities:\n",
      "Repayments of debt —  (700,000) (500,000)\n",
      "Proceeds from issuance of common stock 169,990  35,746  174,414 \n",
      "Repurchases of common stock (6,045,347) —  (600,022)\n",
      "Taxes paid related to net share settlement of equity awards —  —  (224,168)\n",
      "Other financing activities (75,446) —  — \n",
      "Net cash used in financing activities (5,950,803) (664,254) (1,149,776)\n",
      "Effect of exchange rate changes on cash, cash equivalents and restricted cash 82,684  (170,140) (86,740)\n",
      "Net increase (decrease) in cash, cash equivalents and restricted cash 1,947,933  (884,529) (2,183,759)\n",
      "Cash, cash equivalents and restricted cash, beginning of year 5,170,582  6,055,111  8,238,870 \n",
      "Cash, cash equivalents and restricted cash, end of year $ 7,118,515  $ 5,170,582  $ 6,055,111 \n",
      "Supplemental disclosure:\n",
      "Income taxes paid $ 1,154,973  $ 811,720  $ 509,265 \n",
      "Interest paid 684,504  701,693  763,432 \n",
      "See accompanying notes to consolidated financial statements.\n",
      "41\n",
      "[['Assets', '', '', None, '', '', None], ['Current assets:', '', '', None, '', '', None], ['Cash and cash equivalents', '', '$', '7,116,913', '', '$', '5,147,176'], ['Short-term investments', '', '20,973', None, '', '911,276', None], ['Other current assets', '', '2,780,247', None, '', '3,208,021', None], ['Total current assets', '', '9,918,133', None, '', '9,266,473', None], ['Content assets, net', '', '31,658,056', None, '', '32,736,713', None], ['Property and equipment, net', '', '1,491,444', None, '', '1,398,257', None], ['Other non-current assets', '', '5,664,359', None, '', '5,193,325', None], ['Total assets', '', '$', '48,731,992', '', '$', '48,594,768'], ['Liabilities and Stockholders’ Equity', '', '', None, '', '', None], ['Current liabilities:', '', '', None, '', '', None], ['Current content liabilities', '', '$', '4,466,470', '', '$', '4,480,150'], ['Accounts payable', '', '747,412', None, '', '671,513', None], ['Accrued expenses and other liabilities', '', '1,803,960', None, '', '1,514,650', None], ['Deferred revenue', '', '1,442,969', None, '', '1,264,661', None], ['Short-term debt', '', '399,844', None, '', '—', None], ['Total current liabilities', '', '8,860,655', None, '', '7,930,974', None], ['Non-current content liabilities', '', '2,578,173', None, '', '3,081,277', None], ['Long-term debt', '', '14,143,417', None, '', '14,353,076', None], ['Other non-current liabilities', '', '2,561,434', None, '', '2,452,040', None], ['Total liabilities', '', '28,143,679', None, '', '27,817,367', None], ['Commitments and contingencies (Note 8)', '', '', None, '', '', None], ['Stockholders’ equity:', '', '', None, '', '', None], ['Preferred stock, $0.001 par value; 10,000,000 shares authorized at December 31, 2023 and December 31,\\n2022; no shares issued and outstanding at December 31, 2023 and December 31, 2022', '', '—', None, '', '—', None], ['Common stock, $0.001 par value; 4,990,000,000 shares authorized at December 31, 2023 and\\nDecember 31, 2022; 432,759,584 and 445,346,776 issued and outstanding at December 31, 2023 and\\nDecember 31, 2022, respectively', '', '5,145,172', None, '', '4,637,601', None], ['Treasury stock at cost (16,078,268 and 1,564,478 shares at December 31, 2023 and December 31, 2022)', '', '(6,922,200)', None, '', '(824,190)', None], ['Accumulated other comprehensive loss', '', '(223,945)', None, '', '(217,306)', None], ['Retained earnings', '', '22,589,286', None, '', '17,181,296', None], ['Total stockholders’ equity', '', '20,588,313', None, '', '20,777,401', None], ['Total liabilities and stockholders’ equity', '', '$', '48,731,992', '', '$', '48,594,768']]\n",
      "----\n",
      "Table of Contents\n",
      "NETFLIX, INC.\n",
      "CONSOLIDATED BALANCE SHEETS\n",
      "(in thousands, except share and per share data)\n",
      " \n",
      "  As of December 31,\n",
      "   2023 2022\n",
      "Assets\n",
      "Current assets:\n",
      "Cash and cash equivalents $ 7,116,913  $ 5,147,176 \n",
      "Short-term investments 20,973  911,276 \n",
      "Other current assets 2,780,247  3,208,021 \n",
      "Total current assets 9,918,133  9,266,473 \n",
      "Content assets, net 31,658,056  32,736,713 \n",
      "Property and equipment, net 1,491,444  1,398,257 \n",
      "Other non-current assets 5,664,359  5,193,325 \n",
      "Total assets $ 48,731,992  $ 48,594,768 \n",
      "Liabilities and Stockholders’ Equity\n",
      "Current liabilities:\n",
      "Current content liabilities $ 4,466,470  $ 4,480,150 \n",
      "Accounts payable 747,412  671,513 \n",
      "Accrued expenses and other liabilities 1,803,960  1,514,650 \n",
      "Deferred revenue 1,442,969  1,264,661 \n",
      "Short-term debt 399,844  — \n",
      "Total current liabilities 8,860,655  7,930,974 \n",
      "Non-current content liabilities 2,578,173  3,081,277 \n",
      "Long-term debt 14,143,417  14,353,076 \n",
      "Other non-current liabilities 2,561,434  2,452,040 \n",
      "Total liabilities 28,143,679  27,817,367 \n",
      "Commitments and contingencies (Note 8)\n",
      "Stockholders’ equity:\n",
      "Preferred stock, $0.001 par value; 10,000,000 shares authorized at December 31, 2023 and December 31,\n",
      "2022; no shares issued and outstanding at December 31, 2023 and December 31, 2022 —  — \n",
      "Common stock, $0.001 par value; 4,990,000,000 shares authorized at December 31, 2023 and\n",
      "December 31, 2022; 432,759,584 and 445,346,776 issued and outstanding at December 31, 2023 and\n",
      "December 31, 2022, respectively 5,145,172  4,637,601 \n",
      "Treasury stock at cost (16,078,268 and 1,564,478 shares at December 31, 2023 and December 31, 2022) (6,922,200) (824,190)\n",
      "Accumulated other comprehensive loss (223,945) (217,306)\n",
      "Retained earnings 22,589,286  17,181,296 \n",
      "Total stockholders’ equity 20,588,313  20,777,401 \n",
      "Total liabilities and stockholders’ equity $ 48,731,992  $ 48,594,768 \n",
      "See accompanying notes to consolidated financial statements.\n",
      "42\n",
      "[['Balances as of December 31, 2020', '442,895,261', '', '$', '3,447,698', '', '$', '—', '', '$', '44,398', '', '$', '7,573,144', '', '$', '11,065,240'], ['Net income', '—', '', '—', None, '', '—', None, '', '—', None, '', '5,116,228', None, '', '5,116,228', None], ['Other comprehensive loss', '—', '', '—', None, '', '—', None, '', '(84,893)', None, '', '—', None, '', '(84,893)', None], ['Issuance of common stock upon exercise of options', '2,632,324', '', '173,643', None, '', '—', None, '', '—', None, '', '—', None, '', '173,643', None], ['Repurchases of common stock', '(1,182,410)', '', '—', None, '', '(600,022)', None, '', '—', None, '', '—', None, '', '(600,022)', None], ['Shares withheld related to net share settlement', '(382,068)', '', '—', None, '', '(224,168)', None, '', '—', None, '', '—', None, '', '(224,168)', None], ['Stock-based compensation expense', '—', '', '403,220', None, '', '—', None, '', '—', None, '', '—', None, '', '403,220', None], ['Balances as of December 31, 2021', '443,963,107', '', '$', '4,024,561', '', '$', '(824,190)', '', '$', '(40,495)', '', '$', '12,689,372', '', '$', '15,849,248'], ['Net income', '—', '', '—', None, '', '—', None, '', '—', None, '', '4,491,924', None, '', '4,491,924', None], ['Other comprehensive loss', '—', '', '—', None, '', '—', None, '', '(176,811)', None, '', '—', None, '', '(176,811)', None], ['Issuance of common stock upon exercise of options', '1,383,669', '', '37,588', None, '', '—', None, '', '—', None, '', '—', None, '', '37,588', None], ['Stock-based compensation expense', '—', '', '575,452', None, '', '—', None, '', '—', None, '', '—', None, '', '575,452', None], ['Balances as of December 31, 2022', '445,346,776', '', '$', '4,637,601', '', '$', '(824,190)', '', '$', '(217,306)', '', '$', '17,181,296', '', '$', '20,777,401'], ['Net income', '—', '', '—', None, '', '—', None, '', '—', None, '', '5,407,990', None, '', '5,407,990', None], ['Other comprehensive loss', '—', '', '—', None, '', '—', None, '', '(6,639)', None, '', '—', None, '', '(6,639)', None], ['Issuance of common stock upon exercise of options', '1,926,598', '', '168,203', None, '', '—', None, '', '—', None, '', '—', None, '', '168,203', None], ['Repurchases of common stock', '(14,513,790)', '', '—', None, '', '(6,098,010)', None, '', '—', None, '', '—', None, '', '(6,098,010)', None], ['Stock-based compensation expense', '—', '', '339,368', None, '', '—', None, '', '—', None, '', '—', None, '', '339,368', None], ['Balances as of December 31, 2023', '432,759,584', '', '$', '5,145,172', '', '$', '(6,922,200)', '', '$', '(223,945)', '', '$', '22,589,286', '', '$', '20,588,313']]\n",
      "----\n",
      "Table of Contents\n",
      "NETFLIX, INC.\n",
      "CONSOLIDATED STATEMENTS OF STOCKHOLDERS’ EQUITY\n",
      "(in thousands, except share data)\n",
      " \n",
      "Accumulated\n",
      "Other Total\n",
      "Common Stock and Additional Comprehensive Retained Stockholders’\n",
      "  Paid-in Capital Treasury Stock Income (Loss) Earnings Equity\n",
      "  Shares Amount    \n",
      "Balances as of December 31, 2020 442,895,261  $ 3,447,698  $ —  $ 44,398  $ 7,573,144  $ 11,065,240 \n",
      "Net income —  —  —  —  5,116,228  5,116,228 \n",
      "Other comprehensive loss —  —  —  (84,893) —  (84,893)\n",
      "Issuance of common stock upon exercise of options 2,632,324  173,643  —  —  —  173,643 \n",
      "Repurchases of common stock (1,182,410) —  (600,022) —  —  (600,022)\n",
      "Shares withheld related to net share settlement (382,068) —  (224,168) —  —  (224,168)\n",
      "Stock-based compensation expense —  403,220  —  —  —  403,220 \n",
      "Balances as of December 31, 2021 443,963,107  $ 4,024,561  $ (824,190) $ (40,495) $ 12,689,372  $ 15,849,248 \n",
      "Net income —  —  —  —  4,491,924  4,491,924 \n",
      "Other comprehensive loss —  —  —  (176,811) —  (176,811)\n",
      "Issuance of common stock upon exercise of options 1,383,669  37,588  —  —  —  37,588 \n",
      "Stock-based compensation expense —  575,452  —  —  —  575,452 \n",
      "Balances as of December 31, 2022 445,346,776  $ 4,637,601  $ (824,190) $ (217,306) $ 17,181,296  $ 20,777,401 \n",
      "Net income —  —  —  —  5,407,990  5,407,990 \n",
      "Other comprehensive loss —  —  —  (6,639) —  (6,639)\n",
      "Issuance of common stock upon exercise of options 1,926,598  168,203  —  —  —  168,203 \n",
      "Repurchases of common stock (14,513,790) —  (6,098,010) —  —  (6,098,010)\n",
      "Stock-based compensation expense —  339,368  —  —  —  339,368 \n",
      "Balances as of December 31, 2023 432,759,584  $ 5,145,172  $ (6,922,200) $ (223,945) $ 22,589,286  $ 20,588,313 \n",
      "See accompanying notes to consolidated financial statements.\n",
      "43\n",
      "[['Revenues', '', '$', '14,873,783', '', '$', '14,084,643', '', '$', '12,972,100'], ['Paid net membership additions (losses)', '', '5,832', None, '', '(919)', None, '', '1,279', None], ['Paid memberships at end of period (1)', '', '80,128', None, '', '74,296', None, '', '75,215', None]]\n",
      "----\n",
      "Table of Contents\n",
      "are designated as cash flow hedges of foreign currency firm commitments and forecasted transactions and generally have maturities of 24 months or less. The\n",
      "hedging contracts may reduce, but do not entirely eliminate, the effect of foreign currency exchange movements, and the Company may choose not to hedge\n",
      "certain exposures.\n",
      "The Company recognizes derivative instruments at fair value as either assets (presented in “Other current assets” and “Other non-current assets”) or\n",
      "liabilities (presented in “Accrued expenses and other liabilities'' and “Other non-current liabilities”) on the Company’s Consolidated Balance Sheets. The\n",
      "Company classifies derivative instruments in the Level 2 category within the fair value hierarchy.\n",
      "The gain or loss on derivative instruments designated as cash flow hedges of forecasted foreign currency revenue is initially reported as a component of\n",
      "accumulated other comprehensive income (“AOCI\") and reclassified into “Revenues” on the Consolidated Statements of Operations in the same period the\n",
      "forecasted transaction affects earnings. The gain or loss on derivative instruments designated as cash flow hedges of firmly committed or forecasted\n",
      "transactions related to the licensing and production of content assets is initially reported as a component of AOCI and reclassified into “Cost of Revenues” on\n",
      "the Consolidated Statements of Operations in the same period the hedged transaction affects earnings, which occurs as the underlying hedged content assets are\n",
      "amortized. Cash flows from hedging activities are classified in the same category as the cash flows for the underlying item being hedged within \"Net cash\n",
      "provided by operating activities\" on the Consolidated Statements of Cash Flows.\n",
      "In the event that the likelihood of occurrence of the underlying forecasted transactions is determined to be probable not to occur, the gains or losses on the\n",
      "related cash flow hedges are reclassified from AOCI to “Interest and other income (expense)” in the Consolidated Statements of Operations in the period of\n",
      "dedesignation.\n",
      "See Note 7 Derivative Financial Instruments to the consolidated financial statements for further information regarding the Company’s derivative financial\n",
      "instruments.\n",
      "Stock-Based Compensation\n",
      "The Company grants non-qualified stock options to its employees on a monthly basis. Stock-based compensation expense is based on the fair value of\n",
      "the options at the grant date and is recognized, net of forfeitures, over the requisite service period. See Note 9 Stockholders' Equity to the consolidated financial\n",
      "statements for further information regarding stock-based compensation.\n",
      "2. Revenue Recognition\n",
      "The following tables summarize streaming revenues, paid net membership additions (losses), and ending paid memberships by region for the years ended\n",
      "December 31, 2023, 2022 and 2021, respectively:\n",
      "United States and Canada (UCAN)\n",
      "As of/Year Ended December 31,\n",
      "  2023 2022 2021\n",
      "  (in thousands)\n",
      "Revenues $ 14,873,783  $ 14,084,643  $ 12,972,100 \n",
      "Paid net membership additions (losses) 5,832  (919) 1,279 \n",
      "Paid memberships at end of period (1) 80,128  74,296  75,215 \n",
      "Europe, Middle East, and Africa (EMEA)\n",
      "As of/Year Ended December 31,\n",
      "  2023 2022 2021\n",
      "  (in thousands)\n",
      "Revenues $ 10,556,487  $ 9,745,015  $ 9,699,819 \n",
      "Paid net membership additions 12,084  2,693  7,338 \n",
      "Paid memberships at end of period (1) 88,813  76,729  74,036 \n",
      "47\n",
      "[[None, None, '(in thousands)', None, None, None, None, None, None, None], ['Revenues', '', '$', '4,446,461', '', '$', '4,069,973', '', '$', '3,576,976'], ['Paid net membership additions', '', '4,298', None, '', '1,738', None, '', '2,424', None], ['Paid memberships at end of period (1)', '', '45,997', None, '', '41,699', None, '', '39,961', None]]\n",
      "----\n",
      "Table of Contents\n",
      "Latin America (LATAM)\n",
      "As of/Year Ended December 31,\n",
      "  2023 2022 2021\n",
      "  (in thousands)\n",
      "Revenues $ 4,446,461  $ 4,069,973  $ 3,576,976 \n",
      "Paid net membership additions 4,298  1,738  2,424 \n",
      "Paid memberships at end of period (1) 45,997  41,699  39,961 \n",
      "Asia-Pacific (APAC)\n",
      "As of/Year Ended December 31,\n",
      "  2023 2022 2021\n",
      "  (in thousands)\n",
      "Revenues $ 3,763,727  $ 3,570,221  $ 3,266,601 \n",
      "Paid net membership additions 7,315  5,391  7,140 \n",
      "Paid memberships at end of period (1) 45,338  38,023  32,632 \n",
      "(1) A paid membership (also referred to as a paid subscription) is defined as a membership that has the right to receive Netflix service following sign-up and a method of\n",
      "payment being provided, and that is not part of a free trial or certain other promotions that may be offered by the Company to new or rejoining members. Certain members have\n",
      "the option to add extra member sub accounts. These extra member sub accounts are not included in paid memberships. A membership is canceled and ceases to be reflected in\n",
      "the above metrics as of the effective cancellation date. Voluntary cancellations generally become effective at the end of the prepaid membership period. Involuntary\n",
      "cancellations, as a result of a failed method of payment, become effective immediately. Memberships are assigned to territories based on the geographic location used at time of\n",
      "sign-up as determined by the Company’s internal systems, which utilize industry standard geo-location technology.\n",
      "Deferred revenue consists of membership fees billed that have not been recognized, as well as gift and other prepaid memberships that have not been\n",
      "fully redeemed. As of December 31, 2023, total deferred revenue was $1,443 million, the vast majority of which was related to membership fees billed that are\n",
      "expected to be recognized as revenue within the next month. The remaining deferred revenue balance, which is related to gift cards and other prepaid\n",
      "memberships, will be recognized as revenue over the period of service after redemption, which is expected to occur over the next 12 months. The $178 million\n",
      "increase in deferred revenue as compared to the balance of $1,265 million for the year ended December 31, 2022, is a result of the increase in membership fees\n",
      "billed due to increased memberships.\n",
      "48\n",
      "[['Weighted-average shares of common stock outstanding', '441,571', None, '', '444,698', None, '', '443,155', None], ['Employee stock options', '7,927', None, '', '6,592', None, '', '12,217', None], ['Weighted-average number of shares', '449,498', None, '', '451,290', None, '', '455,372', None], ['Diluted earnings per share', '$', '12.03', '', '$', '9.95', '', '$', '11.24']]\n",
      "----\n",
      "Table of Contents\n",
      "3. Earnings per Share\n",
      "Basic earnings per share is computed using the weighted-average number of outstanding shares of common stock during the period. Diluted earnings per\n",
      "share is computed using the weighted-average number of outstanding shares of common stock and, when dilutive, potential common shares outstanding during\n",
      "the period. Potential common shares consist of incremental shares issuable upon the assumed exercise of stock options. The computation of earnings per share\n",
      "is as follows:\n",
      " \n",
      "  Year Ended December 31,\n",
      "  2023 2022 2021\n",
      "  (in thousands, except per share data)\n",
      "Basic earnings per share:\n",
      "Net income $ 5,407,990  $ 4,491,924  $ 5,116,228 \n",
      "Shares used in computation:\n",
      "Weighted-average shares of common stock outstanding 441,571  444,698  443,155 \n",
      "Basic earnings per share $ 12.25  $ 10.10  $ 11.55 \n",
      "Diluted earnings per share:\n",
      "Net income $ 5,407,990  $ 4,491,924  $ 5,116,228 \n",
      "Shares used in computation:\n",
      "Weighted-average shares of common stock outstanding 441,571  444,698  443,155 \n",
      "Employee stock options 7,927  6,592  12,217 \n",
      "Weighted-average number of shares 449,498  451,290  455,372 \n",
      "Diluted earnings per share $ 12.03  $ 9.95  $ 11.24 \n",
      "Employee stock options with exercise prices greater than the average market price of the common stock were excluded from the diluted calculation as\n",
      "their inclusion would have been anti-dilutive. The following table summarizes the potential common shares excluded from the diluted calculation:\n",
      " \n",
      "  Year Ended December 31,\n",
      "  2023 2022 2021\n",
      "  (in thousands)\n",
      "Employee stock options 4,109  6,790  348 \n",
      "49\n",
      "[['Cash', '$', '5,986,629', '$', '—', '$', '1,466', '$', '81', '$', '5,988,176'], ['Level 1 securities:', '', None, '', None, '', None, '', None, '', None], ['Money market funds', '925,652', None, '—', None, '—', None, '55', None, '925,707', None], ['Level 2 securities:', '', None, '', None, '', None, '', None, '', None], ['Time Deposits (1)', '204,632', None, '20,973', None, '—', None, '—', None, '225,605', None], ['', '', None, '', None, '', None, '', None, '', None], ['', '$', '7,116,913', '$', '20,973', '$', '1,466', '$', '136', '$', '7,139,488']]\n",
      "----\n",
      "Table of Contents\n",
      "4. Cash, Cash Equivalents, Restricted Cash, and Short-term Investments\n",
      "The Company classifies short-term investments, which consist of marketable securities with original maturities in excess of 90 days as available-for-sale.\n",
      "The Company does not buy and hold securities principally for the purpose of selling them in the near future. The Company’s policy is focused on the\n",
      "preservation of capital, liquidity and return. From time to time, the Company may sell certain securities but the objectives are generally not to generate profits\n",
      "on short-term differences in price.\n",
      "The following tables summarize the Company's cash, cash equivalents, restricted cash and short-term investments as of December 31, 2023 and 2022:\n",
      "  As of December 31, 2023\n",
      "Cash and cash Short-term Other Current Non-current\n",
      "  equivalents investments Assets Assets Total\n",
      "  (in thousands)\n",
      "Cash $ 5,986,629  $ —  $ 1,466  $ 81  $ 5,988,176 \n",
      "Level 1 securities:\n",
      "Money market funds 925,652  —  —  55  925,707 \n",
      "Level 2 securities:\n",
      "Time Deposits (1) 204,632  20,973  —  —  225,605 \n",
      "$ 7,116,913  $ 20,973  $ 1,466  $ 136  $ 7,139,488 \n",
      "(1) The majority of the Company's time deposits are international deposits, which mature within one year.\n",
      "  As of December 31, 2022\n",
      "Cash and cash Short-term Other Current Non-current\n",
      "  equivalents investments Assets Assets Total\n",
      "  (in thousands)\n",
      "Cash $ 4,071,584  $ —  $ 3,410  $ 19,874  $ 4,094,868 \n",
      "Level 1 securities:\n",
      "Money market funds 569,826  —  —  122  569,948 \n",
      "Level 2 securities:\n",
      "Time Deposits (2) 505,766  911,276  —  —  1,417,042 \n",
      "$ 5,147,176  $ 911,276  $ 3,410  $ 19,996  $ 6,081,858 \n",
      "(2) The majority of the Company's time deposits are domestic deposits, which mature within one year.\n",
      "Other current assets include restricted cash for deposits related to self-insurance and letter of credit agreements. Non-current assets include restricted cash\n",
      "related to letter of credit agreements. The fair value of cash equivalents and short-term investments included in the Level 2 category is based on observable\n",
      "inputs, such as quoted prices for similar assets at the measurement date; quoted prices in markets that are not active; or other inputs that are observable, either\n",
      "directly or indirectly.\n",
      "See Note 6 Debt to the consolidated financial statements for further information regarding the fair value of the Company’s senior notes.\n",
      "There were no material gross realized gains or losses for the years ended December 31, 2023 and 2022.\n",
      "50\n",
      "[['Licensed content, net', '$', '12,722,701', '', '$', '12,732,549'], ['', '', None, '', '', None], ['Produced content, net', '', None, '', '', None], ['Released, less amortization', '9,843,150', None, '', '9,110,518', None], ['In production', '8,247,578', None, '', '10,255,940', None], ['In development and pre-production', '844,627', None, '', '637,706', None], ['', '18,935,355', None, '', '20,004,164', None], ['', '', None, '', '', None], ['Content assets, net', '$', '31,658,056', '', '$', '32,736,713']]\n",
      "----\n",
      "Table of Contents\n",
      "5. Balance Sheet Components\n",
      "Content Assets, Net\n",
      "Content assets consisted of the following:\n",
      "As of December 31,\n",
      "2023 2022\n",
      "(in thousands)\n",
      "Licensed content, net $ 12,722,701  $ 12,732,549 \n",
      "Produced content, net\n",
      "Released, less amortization 9,843,150  9,110,518 \n",
      "In production 8,247,578  10,255,940 \n",
      "In development and pre-production 844,627  637,706 \n",
      "18,935,355  20,004,164 \n",
      "Content assets, net $ 31,658,056  $ 32,736,713 \n",
      "As of December 31, 2023, approximately $5,777 million, $2,860 million, and $1,842 million of the $12,723 million unamortized cost of the licensed\n",
      "content is expected to be amortized in each of the next three years. As of December 31, 2023, approximately $3,766 million, $2,622 million, and $1,793\n",
      "million of the $9,843 million unamortized cost of the produced content that has been released is expected to be amortized in each of the next three years.\n",
      "As of December 31, 2023, the amount of accrued participations and residuals was not material.\n",
      "The following table represents the amortization of content assets:\n",
      "Year Ended December 31,\n",
      "  2023 2022 2021\n",
      "(in thousands)\n",
      "Licensed content $ 7,145,446  $ 7,681,978  $ 8,055,811 \n",
      "Produced content (1) 7,051,991  6,344,154  4,174,556 \n",
      "Total $ 14,197,437  $ 14,026,132  $ 12,230,367 \n",
      "(1) Tax incentives earned on qualified production spend generally reduce the cost-basis of content assets and result in lower content amortization over the\n",
      "life of the title. For the years ended December 31, 2023 and 2022, tax incentives resulted in lower content amortization on produced content of approximately\n",
      "$835 million and $719 million, respectively.\n",
      "Property and Equipment, Net\n",
      "51\n",
      "[['Land', '', '$', '88,429', '', '$', '85,005', ''], ['Buildings', '', '150,736', None, '', '52,106', None, '30 years'], ['Leasehold improvements', '', '1,032,492', None, '', '1,040,570', None, 'Over life of lease'], ['Furniture and fixtures', '', '144,737', None, '', '153,682', None, '3 years'], ['Information technology', '', '414,092', None, '', '442,681', None, '3 years'], ['Corporate aircraft', '', '99,175', None, '', '115,578', None, '8-10 years'], ['Machinery and equipment', '', '10,334', None, '', '26,821', None, '3-5 years'], ['Capital work-in-progress', '', '406,492', None, '', '235,555', None, ''], ['Property and equipment, gross', '', '2,346,487', None, '', '2,151,998', None, ''], ['Less: Accumulated depreciation', '', '(855,043)', None, '', '(753,741)', None, ''], ['Property and equipment, net', '', '$', '1,491,444', '', '$', '1,398,257', '']]\n",
      "----\n",
      "Table of Contents\n",
      "Property and equipment and accumulated depreciation consisted of the following:\n",
      "As of December 31,\n",
      "Estimated Useful Lives (in\n",
      "2023 2022 Years)\n",
      "(in thousands)\n",
      "Land $ 88,429  $ 85,005 \n",
      "Buildings 150,736  52,106  30 years\n",
      "Leasehold improvements 1,032,492  1,040,570  Over life of lease\n",
      "Furniture and fixtures 144,737  153,682  3 years\n",
      "Information technology 414,092  442,681  3 years\n",
      "Corporate aircraft 99,175  115,578  8-10 years\n",
      "Machinery and equipment 10,334  26,821  3-5 years\n",
      "Capital work-in-progress 406,492  235,555 \n",
      "Property and equipment, gross 2,346,487  2,151,998 \n",
      "Less: Accumulated depreciation (855,043) (753,741)\n",
      "Property and equipment, net $ 1,491,444  $ 1,398,257 \n",
      "    \n",
      "Leases\n",
      "The Company has entered into operating leases primarily for real estate. These leases generally have terms which range from 1 year to 15 years, and often\n",
      "include one or more options to renew. These renewal terms can extend the lease term from 1 year to 20 years, and are included in the lease term when it is\n",
      "reasonably certain that the Company will exercise the option. These operating leases are included in \"Other non-current assets\" on the Company's Consolidated\n",
      "Balance Sheets, and represent the Company’s right to use the underlying asset for the lease term. The Company’s obligations to make lease payments are\n",
      "included in \"Accrued expenses and other liabilities\" and \"Other non-current liabilities\" on the Company's Consolidated Balance Sheets.  Operating lease right-\n",
      "of-use assets and liabilities are recognized at the commencement date based on the present value of lease payments over the lease term. The Company has\n",
      "entered into various short-term operating leases with an initial term of twelve months or less. These leases are not recorded on the Company's Consolidated\n",
      "Balance Sheets. All operating lease expense is recognized on a straight-line basis over the lease term. Because the rate implicit in each lease is not readily\n",
      "determinable, the Company uses its incremental borrowing rate to determine the present value of the lease payments. The Company has certain contracts for\n",
      "real estate which may contain lease and non-lease components which it has elected to treat as a single lease component.\n",
      "The components of lease costs for the years ended December 31, 2023, 2022 and 2021 were as follows:\n",
      "Year ended December 31,\n",
      "  2023 2022 2021\n",
      "(in thousands)\n",
      "Operating lease cost $ 430,856  $ 413,664  $ 389,805 \n",
      "Short-term lease cost 207,822  194,764  152,765 \n",
      "Total lease cost $ 638,678  $ 608,428  $ 542,570 \n",
      "Information related to the Company's operating right-of-use assets and related operating lease liabilities were as follows:\n",
      "Year ended December 31,\n",
      "2023 2022 2021\n",
      "(in thousands)\n",
      "Cash paid for operating lease liabilities $ 451,525  $ 413,034  $ 349,586 \n",
      "Right-of-use assets obtained in exchange for new operating lease obligations 196,639  252,393  764,142 \n",
      "52\n",
      "[['Operating lease right-of-use assets, net', '$', '2,076,899', '', '$', '2,227,122', ''], ['', '', None, None, '', None, None], ['Current operating lease liabilities', '$', '383,312', '', '$', '355,985', ''], ['Non-current operating lease liabilities', '2,046,801', None, '', '2,222,503', None, ''], ['Total operating lease liabilities', '$', '2,430,113', '', '$', '2,578,488', ''], ['', '', None, None, '', None, None], ['Weighted-average remaining lease term', '7.5 years', None, None, '8.3 years', None, None], ['Weighted-average discount rate', '3.3', None, '%', '3.2', None, '%']]\n",
      "----\n",
      "Table of Contents\n",
      "As of December 31,\n",
      "2023 2022\n",
      "(in thousands, except lease term and discount rate)\n",
      "Operating lease right-of-use assets, net $ 2,076,899  $ 2,227,122 \n",
      "Current operating lease liabilities $ 383,312  $ 355,985 \n",
      "Non-current operating lease liabilities 2,046,801  2,222,503 \n",
      "Total operating lease liabilities $ 2,430,113  $ 2,578,488 \n",
      "Weighted-average remaining lease term 7.5 years 8.3 years\n",
      "Weighted-average discount rate 3.3 % 3.2 %\n",
      "Maturities of operating lease liabilities as of December 31, 2023 were as follows (in thousands):\n",
      "Due in 12 month period ended December 31,\n",
      "2024 $ 460,353 \n",
      "2025 424,897 \n",
      "2026 401,306 \n",
      "2027 340,245 \n",
      "2028 282,465 \n",
      "Thereafter 827,117 \n",
      "2,736,383 \n",
      "Less imputed interest (306,270)\n",
      "Total operating lease liabilities$ 2,430,113 \n",
      "The Company has additional operating leases for real estate of $343 million which have not commenced as of December 31, 2023, and as such, have not\n",
      "been recognized on the Company's Consolidated Balance Sheets. These operating leases are expected to commence in 2024 with lease terms between 2 and 11\n",
      "years.\n",
      "Other Current Assets\n",
      "Other current assets consisted of the following:\n",
      "As of\n",
      "December 31, December 31,\n",
      "2023 2022\n",
      "(in thousands)\n",
      "Trade receivables $ 1,287,054  $ 988,898 \n",
      "Prepaid expenses 408,936  392,735 \n",
      "Other (1) 1,084,257  1,826,388 \n",
      "Total other current assets $ 2,780,247  $ 3,208,021 \n",
      "(1) $555 million and $598 million of receivables related to tax incentives earned on production spend are included in Other as of December 31, 2023 and\n",
      "2022, respectively.\n",
      "The decrease in Other was primarily driven by receipt of amounts due under a modified content licensing arrangement.\n",
      "53\n",
      "[['5.750% Senior Notes', '$', '400', '$', '400', '', 'February 2014', '', 'March 2024', '', '$', '400', '$', '404'], ['5.875% Senior Notes', '800', None, '800', None, '', 'February 2015', '', 'February 2025', '', '807', None, '811', None], ['3.000% Senior Notes (1)', '519', None, '503', None, '', 'April 2020', '', 'June 2025', '', '516', None, '495', None], ['3.625% Senior Notes', '500', None, '500', None, '', 'April 2020', '', 'June 2025', '', '491', None, '479', None], ['4.375% Senior Notes', '1,000', None, '1,000', None, '', 'October 2016', '', 'November 2026', '', '996', None, '980', None], ['3.625% Senior Notes (1)', '1,434', None, '1,391', None, '', 'May 2017', '', 'May 2027', '', '1,454', None, '1,338', None], ['4.875% Senior Notes', '1,600', None, '1,600', None, '', 'October 2017', '', 'April 2028', '', '1,621', None, '1,557', None], ['5.875% Senior Notes', '1,900', None, '1,900', None, '', 'April 2018', '', 'November 2028', '', '2,009', None, '1,930', None], ['4.625% Senior Notes (1)', '1,215', None, '1,177', None, '', 'October 2018', '', 'May 2029', '', '1,300', None, '1,151', None], ['6.375% Senior Notes', '800', None, '800', None, '', 'October 2018', '', 'May 2029', '', '872', None, '830', None], ['3.875% Senior Notes (1)', '1,325', None, '1,284', None, '', 'April 2019', '', 'November 2029', '', '1,372', None, '1,201', None], ['5.375% Senior Notes', '900', None, '900', None, '', 'April 2019', '', 'November 2029', '', '931', None, '885', None], ['3.625% Senior Notes (1)', '1,215', None, '1,177', None, '', 'October 2019', '', 'June 2030', '', '1,237', None, '1,078', None], ['4.875% Senior Notes', '1,000', None, '1,000', None, '', 'October 2019', '', 'June 2030', '', '1,012', None, '944', None], ['', '$', '14,608', '$', '14,432', '', '', '', '', '', '$', '15,018', '$', '14,083']]\n",
      "----\n",
      "Table of Contents\n",
      "6. Debt\n",
      "As of December 31, 2023, the Company had aggregate outstanding notes of $14,543 million, net of $65 million of issuance costs, with varying maturities\n",
      "(the \"Notes\"). Of the outstanding balance, $400 million, net of issuance costs, is classified as short-term debt on the Consolidated Balance Sheets. As of\n",
      "December 31, 2022, the Company had aggregate outstanding long-term notes of $14,353 million, net of $79 million of issuance costs. Each of the Notes were\n",
      "issued at par and are senior unsecured obligations of the Company. Interest is payable semi-annually at fixed rates. A portion of the outstanding Notes is\n",
      "denominated in foreign currency (comprised of €5,170 million) and is remeasured into U.S. dollars at each balance sheet date (with remeasurement loss\n",
      "totaling $176 million for the year ended December 31, 2023).\n",
      "The following table provides a summary of the Company's outstanding debt and the fair values based on quoted market prices in less active markets as of\n",
      "December 31, 2023 and December 31, 2022:\n",
      "Principal Amount at Par Level 2 Fair Value as of\n",
      "December 31, December 31, December 31, December 31,\n",
      "2023 2022 Issuance Date Maturity 2023 2022\n",
      "(in millions) (in millions)\n",
      "5.750% Senior Notes $ 400  $ 400  February 2014 March 2024 $ 400  $ 404 \n",
      "5.875% Senior Notes 800  800  February 2015 February 2025 807  811 \n",
      "3.000% Senior Notes (1) 519  503  April 2020 June 2025 516  495 \n",
      "3.625% Senior Notes 500  500  April 2020 June 2025 491  479 \n",
      "4.375% Senior Notes 1,000  1,000  October 2016 November 2026 996  980 \n",
      "3.625% Senior Notes (1) 1,434  1,391  May 2017 May 2027 1,454  1,338 \n",
      "4.875% Senior Notes 1,600  1,600  October 2017 April 2028 1,621  1,557 \n",
      "5.875% Senior Notes 1,900  1,900  April 2018 November 2028 2,009  1,930 \n",
      "4.625% Senior Notes (1) 1,215  1,177  October 2018 May 2029 1,300  1,151 \n",
      "6.375% Senior Notes 800  800  October 2018 May 2029 872  830 \n",
      "3.875% Senior Notes (1) 1,325  1,284  April 2019 November 2029 1,372  1,201 \n",
      "5.375% Senior Notes 900  900  April 2019 November 2029 931  885 \n",
      "3.625% Senior Notes (1) 1,215  1,177  October 2019 June 2030 1,237  1,078 \n",
      "4.875% Senior Notes 1,000  1,000  October 2019 June 2030 1,012  944 \n",
      "$ 14,608  $ 14,432  $ 15,018  $ 14,083 \n",
      "(1) The following Senior Notes have a principal amount denominated in euro: 3.000% Senior Notes for €470 million, 3.625% Senior Notes for €1,300 million,\n",
      "4.625% Senior Notes for €1,100 million, 3.875% Senior Notes for €1,200 million, and 3.625% Senior Notes for €1,100 million.\n",
      "Each of the Notes are repayable in whole or in part upon the occurrence of a change of control, at the option of the holders, at a purchase price in cash\n",
      "equal to 101% of the principal plus accrued interest. The Company may redeem the Notes prior to maturity in whole or in part at an amount equal to the\n",
      "principal amount thereof plus accrued and unpaid interest and an applicable premium. The Notes include, among other terms and conditions, limitations on the\n",
      "Company's ability to create, incur or allow certain liens; enter into sale and lease-back transactions; create, assume, incur or guarantee additional indebtedness\n",
      "of certain of the Company's subsidiaries; and consolidate or merge with, or convey, transfer or lease all or substantially all of the Company's and its subsidiaries\n",
      "assets, to another person. As of December 31, 2023 and December 31, 2022, the Company was in compliance with all related covenants.\n",
      "Revolving Credit Facility\n",
      "On March 6, 2023, the Company amended its $1 billion unsecured revolving credit facility (\"Revolving Credit Agreement\") to replace the London\n",
      "interbank offered rate to a variable secured overnight financing rate (the “Term SOFR Rate”) as the rate to which interest payments are indexed, among other\n",
      "things. The Revolving Credit Agreement matures on June 17, 2026. Revolving loans may be borrowed, repaid and reborrowed until June 17, 2026, at which\n",
      "time all amounts borrowed must be repaid. The Company may use the proceeds of future borrowings under the Revolving Credit Agreement for\n",
      "54\n",
      "[['Derivatives designated as hedging instruments:', '', None, '', None, '', None, '', None], ['Foreign exchange contracts', '$', '26,416', '$', '4,518', '$', '140,089', '$', '46,575'], ['Total', '$', '26,416', '$', '4,518', '$', '140,089', '$', '46,575']]\n",
      "----\n",
      "Table of Contents\n",
      "working capital and general corporate purposes. As of December 31, 2023, no amounts have been borrowed under the Revolving Credit Agreement.\n",
      "The borrowings under the Revolving Credit Agreement bear interest, at the Company’s option, of either (i) a floating rate equal to a base rate (the\n",
      "“Alternate Base Rate”) or (ii) a rate equal to the Term SOFR Rate (or the applicable benchmark replacement), plus a margin of 0.75%. The Alternate Base Rate\n",
      "is defined as the greatest of (A) the rate of interest published by the Wall Street Journal, from time to time, as the prime rate, (B) the federal funds rate,\n",
      "plus 0.50% and (C) the Term SOFR Rate for a one-month tenor, plus 1.00%. The Term SOFR Rate is the forward-looking secured overnight financing rate\n",
      "administered by the Federal Reserve Bank of New York or a successor administrator, for the relevant interest period, but in no event shall the Term SOFR Rate\n",
      "be less than 0.00% per annum.\n",
      "The Company is also obligated to pay a commitment fee on the undrawn amounts of the Revolving Credit Agreement at an annual rate of 0.10%. The\n",
      "Revolving Credit Agreement requires the Company to comply with certain covenants, including covenants that limit or restrict the ability of the Company’s\n",
      "subsidiaries to incur debt and limit or restrict the ability of the Company and its subsidiaries to grant liens and enter into sale and leaseback transactions; and, in\n",
      "the case of the Company or a guarantor, merge, consolidate, liquidate, dissolve or sell, transfer, lease or otherwise dispose of all or substantially all of the assets\n",
      "of the Company and its subsidiaries, taken as a whole. As of December 31, 2023 and December 31, 2022, the Company was in compliance with all related\n",
      "covenants.\n",
      "7. Derivative Financial Instruments\n",
      "In the fiscal year ended December 31, 2023, the Company entered into derivative financial instruments to manage foreign exchange risk related to its\n",
      "ongoing business operations with the primary objective of reducing operating income and cash flow volatility associated with fluctuations in foreign exchange\n",
      "rates. The Company did not use any derivative instruments prior to the fiscal year ended December 31, 2023.\n",
      "Notional Amount of Derivative Contracts\n",
      "The net notional amounts of the Company’s outstanding derivative instruments were as follows:\n",
      "As of December 31,\n",
      "2023 2022\n",
      "(in thousands)\n",
      "Derivatives designated as hedging instruments:\n",
      "Foreign exchange contracts\n",
      "Cash flow hedges $ 8,783,273  $ — \n",
      "Total $ 8,783,273  $ — \n",
      "Fair Value of Derivative Contracts\n",
      "The fair value of the Company’s outstanding derivative instruments were as follows:\n",
      "  As of December 31, 2023\n",
      "Derivative Assets Derivative Liabilities\n",
      "Other non-current Accrued expenses and Other non-current\n",
      "  Other current assets assets other liabilities liabilities\n",
      "  (in thousands)\n",
      "Derivatives designated as hedging instruments:\n",
      "Foreign exchange contracts $ 26,416  $ 4,518  $ 140,089  $ 46,575 \n",
      "Total $ 26,416  $ 4,518  $ 140,089  $ 46,575 \n",
      "The Company classifies derivative instruments in the Level 2 category within the fair value hierarchy. These instruments are valued using industry\n",
      "standard valuation models that use observable inputs such as interest rate yield curves, and forward and spot prices for currencies.\n",
      "As of December 31, 2023, the pre-tax net accumulated loss on our foreign currency cash flow hedges included in AOCI on the Consolidated Balance\n",
      "Sheets expected to be recognized in earnings within the next 12 months is $128 million.\n",
      "55\n",
      "[['Derivative assets', '$', '30,934', '', '$', '—', '', '$', '30,934', '', '$', '(27,246)', '', '$', '—', '', '$', '3,688'], ['Derivative liabilities', '186,664', None, '', '—', None, '', '186,664', None, '', '(27,246)', None, '', '—', None, '', '159,418', None]]\n",
      "----\n",
      "Table of Contents\n",
      "Master Netting Agreements\n",
      "In order to mitigate counterparty credit risk, the Company enters into master netting agreements with its counterparties for its foreign currency exchange\n",
      "contracts, which permit the parties to settle amounts on a net basis under certain conditions. The Company has elected to present its derivative assets and\n",
      "liabilities on a gross basis on its Consolidated Balance Sheets.\n",
      "The Company also enters into collateral security arrangements with its counterparties that require the parties to post cash collateral when certain\n",
      "contractual thresholds are met. No cash collateral was received or posted by the Company as of December 31, 2023.\n",
      "The potential offsetting effect to the Company’s derivative assets and liabilities under its master netting agreements and collateral security agreements\n",
      "were as follows:\n",
      "  As of December 31, 2023\n",
      "Gross Amount Not Offset in the\n",
      "Consolidated Balance Sheets\n",
      "Gross Amount Net Amount\n",
      "Recognized in the Gross Amount Offset Presented in the\n",
      "Consolidated in the Consolidated Consolidated Financial Collateral Received\n",
      "  Balance Sheets Balance Sheets Balance Sheets Instruments and Posted Net Amount\n",
      "  (in thousands)\n",
      "Derivative assets $ 30,934  $ —  $ 30,934  $ (27,246) $ —  $ 3,688 \n",
      "Derivative liabilities 186,664  —  186,664  (27,246) —  159,418 \n",
      "Effect of Derivative Instruments on Consolidated Financial Statements\n",
      "The pre-tax gains (losses) on the Company’s cash flow hedges recognized in AOCI were as follows:\n",
      "Year Ended December 31,\n",
      "2023 2022 2021\n",
      "(in thousands)\n",
      "Cash flow hedges:\n",
      "Foreign exchange contracts (1)\n",
      "Amount included in the assessment of effectiveness $ (155,730) $ —  $ — \n",
      "Total $ (155,730) $ —  $ — \n",
      "(1) No amounts were excluded from the assessment of effectiveness.\n",
      "No gains or losses on derivative instruments were reclassified from AOCI into the Consolidated Statements of Operations in the year ended December 31,\n",
      "2023.\n",
      "8.  Commitments and Contingencies\n",
      "Content\n",
      "At December 31, 2023, the Company had $21.7 billion of obligations comprised of $4.5 billion included in \"Current content liabilities\" and $2.6 billion\n",
      "of \"Non-current content liabilities\" on the Consolidated Balance Sheets and $14.6 billion of obligations that are not reflected on the Consolidated Balance\n",
      "Sheets as they did not yet meet the criteria for recognition.\n",
      "At December 31, 2022, the Company had $21.8 billion of obligations comprised of $4.5 billion included in \"Current content liabilities\" and $3.1 billion\n",
      "of \"Non-current content liabilities\" on the Consolidated Balance Sheets and $14.2 billion of obligations that are not reflected on the Consolidated Balance\n",
      "Sheets as they did not yet meet the criteria for recognition.\n",
      "The expected timing of payments for these content obligations is as follows:\n",
      "56\n",
      "[['Less than one year', '$', '10,328,923', '$', '10,038,483'], [None, '8,784,302', None, None, None], ['Due after 3 years and through 5 years', '2,016,358', None, '2,124,307', None], [None, '583,766', None, None, None], ['Total content obligations', '$', '21,713,349', '$', '21,831,947']]\n",
      "----\n",
      "Table of Contents\n",
      "As of December 31,\n",
      "2023 2022\n",
      "  (in thousands)\n",
      "Less than one year $ 10,328,923  $ 10,038,483 \n",
      "Due after one year and through 3 years 8,784,302  9,425,551 \n",
      "Due after 3 years and through 5 years 2,016,358  2,124,307 \n",
      "Due after 5 years 583,766  243,606 \n",
      "Total content obligations $ 21,713,349  $ 21,831,947 \n",
      "    \n",
      "Content obligations include amounts related to the acquisition, licensing and production of content. Obligations that are in non-U.S. dollar currencies are\n",
      "translated to the U.S. dollar at period end rates. An obligation for the production of content includes non-cancelable commitments under creative talent and\n",
      "employment agreements as well as other production related commitments. An obligation for the acquisition and licensing of content is incurred at the time the\n",
      "Company enters into an agreement to obtain future titles. Once a title becomes available, a content liability is recorded on the Consolidated Balance Sheets.\n",
      "Certain agreements include the obligation to license rights for unknown future titles, the ultimate quantity and/or fees for which are not yet determinable as of\n",
      "the reporting date. Traditional film output deals, or certain TV series license agreements where the number of seasons to be aired is unknown, are examples of\n",
      "such license agreements. The Company does not include any estimated obligation for these future titles beyond the known minimum amount. However, the\n",
      "unknown obligations are expected to be significant.\n",
      "Legal Proceedings\n",
      "From time to time, in the normal course of its operations, the Company is subject to litigation matters and claims, including claims relating to employee\n",
      "relations, business practices and patent infringement. Litigation can be expensive and disruptive to normal business operations. Moreover, the results of\n",
      "complex legal proceedings are difficult to predict and the Company's view of these matters may change in the future as the litigation and events related thereto\n",
      "unfold. The Company expenses legal fees as incurred. The Company records a provision for contingent losses when it is both probable that a liability has been\n",
      "incurred and the amount of the loss can be reasonably estimated. An unfavorable outcome to any legal matter, if material, could have an adverse effect on the\n",
      "Company's operations or its financial position, liquidity or results of operations.\n",
      "The Company is involved in litigation matters not listed herein but does not consider the matters to be material either individually or in the aggregate at\n",
      "this time. The Company's view of the matters not listed may change in the future as the litigation and events related thereto unfold.\n",
      "Non-Income Taxes\n",
      "The Company is routinely under audit by various tax authorities with regard to non-income tax matters. The subject matter of non-income tax audits\n",
      "primarily arises from disputes on the tax treatment and tax rate applied to our revenue in certain jurisdictions. We accrue non-income taxes that may result from\n",
      "examinations by, or any negotiated agreements with, these tax authorities when a loss is probable and reasonably estimable.\n",
      "Similar to other U.S. companies doing business in Brazil, the Company is involved in a number of matters with Brazilian tax authorities regarding non-\n",
      "income tax assessments. Although the Company believes it has meritorious defenses to these matters, there is inherent complexity and uncertainty with respect\n",
      "to these matters, and the final outcome may be materially different from our expectations. The current potential exposure with respect to the various issues with\n",
      "Brazilian tax authorities regarding non-income tax assessments is estimated to be approximately $300 million, which is expected to increase over time.\n",
      "Guarantees—Indemnification Obligations\n",
      "In the ordinary course of business, the Company has entered into contractual arrangements under which it has agreed to provide indemnification of\n",
      "varying scope and terms to business partners and other parties with respect to certain matters, including, but not limited to, losses arising out of the Company’s\n",
      "breach of such agreements and out of intellectual property infringement claims made by third parties. In these circumstances, payment may be conditional on\n",
      "the other party making a claim pursuant to the procedures specified in the particular contract.\n",
      "The Company’s obligations under these agreements may be limited in terms of time or amount, and in some instances, the Company may have recourse\n",
      "against third parties for certain payments. In addition, the Company has entered into indemnification agreements with its directors and certain of its officers that\n",
      "will require it, among other things, to indemnify\n",
      "57\n",
      "[['Balances as of December 31, 2020', '21,702,085', '', '18,676,810', '$', '170.23', '', '', '', '', None], ['Granted', '(1,556,725)', '', '1,556,725', '554.11', None, '', '', '', '', None], ['Exercised', '—', '', '(2,632,324)', '65.97', None, '', '', '', '', None], ['Expired', '—', '', '(5,360)', '34.63', None, '', '', '', '', None], ['Balances as of December 31, 2021', '20,145,360', '', '17,595,851', '$', '219.83', '', '', '', '', None], ['Granted', '(3,691,257)', '', '3,691,257', '267.94', None, '', '', '', '', None], ['Exercised', '—', '', '(1,383,669)', '27.19', None, '', '', '', '', None], ['Expired', '—', '', '(6,578)', '11.10', None, '', '', '', '', None], ['Balances as of December 31, 2022', '16,454,103', '', '19,896,861', '$', '242.22', '', '', '', '', None], ['Granted', '(1,729,218)', '', '1,729,218', '372.49', None, '', '', '', '', None], ['Exercised', '—', '', '(1,926,598)', '87.30', None, '', '', '', '', None], ['Expired', '—', '', '(4,372)', '36.39', None, '', '', '', '', None], ['Balances as of December 31, 2023', '14,724,885', '', '19,695,109', '$', '268.86', '', '5.35', '', '$', '4,429,404'], ['Vested and expected to vest as of December 31, 2023', '', '', '19,695,109', '$', '268.86', '', '5.35', '', '$', '4,429,404'], ['Exercisable as of December 31, 2023', '', '', '19,447,739', '$', '267.37', '', '5.30', '', '$', '4,404,586']]\n",
      "----\n",
      "Table of Contents\n",
      "9. Stockholders’ Equity\n",
      "Voting Rights\n",
      "The holders of each share of common stock shall be entitled to one vote per share on all matters to be voted upon by the Company’s stockholders.\n",
      "Stock Option Plan\n",
      "In June 2020, the Company's stockholders approved the 2020 Stock Plan, which was adopted by the Company's Board of Directors in March 2020\n",
      "subject to stockholder approval. The 2020 Stock Plan provides for the grant of incentive stock options to employees and for the grant of non-statutory stock\n",
      "options, stock appreciation rights, restricted stock and restricted stock units to employees, directors and consultants.\n",
      "A summary of the activities related to the Company’s stock option plans is as follows:\n",
      " \n",
      "  Options Outstanding\n",
      "Weighted- Average\n",
      "Shares  Weighted- Average Remaining Aggregate\n",
      "Available Number of Exercise Price Contractual Term Intrinsic Value\n",
      "  for Grant Shares (per share) (in years) (in thousands)\n",
      "Balances as of December 31, 2020 21,702,085  18,676,810  $ 170.23 \n",
      "Granted (1,556,725) 1,556,725  554.11 \n",
      "Exercised —  (2,632,324) 65.97 \n",
      "Expired —  (5,360) 34.63 \n",
      "Balances as of December 31, 2021 20,145,360  17,595,851  $ 219.83 \n",
      "Granted (3,691,257) 3,691,257  267.94 \n",
      "Exercised —  (1,383,669) 27.19 \n",
      "Expired —  (6,578) 11.10 \n",
      "Balances as of December 31, 2022 16,454,103  19,896,861  $ 242.22 \n",
      "Granted (1,729,218) 1,729,218  372.49 \n",
      "Exercised —  (1,926,598) 87.30 \n",
      "Expired —  (4,372) 36.39 \n",
      "Balances as of December 31, 2023 14,724,885  19,695,109  $ 268.86  5.35 $ 4,429,404 \n",
      "Vested and expected to vest as of December 31, 2023 19,695,109  $ 268.86  5.35 $ 4,429,404 \n",
      "Exercisable as of December 31, 2023 19,447,739  $ 267.37  5.30 $ 4,404,586 \n",
      "The aggregate intrinsic value in the table above represents the total pretax intrinsic value (the difference between the Company’s closing stock price on\n",
      "the last trading day of 2023 and the exercise price, multiplied by the number of in-the-money options) that would have been received by the option holders had\n",
      "all option holders exercised their options on the last trading day of 2023. This amount changes based on the fair market value of the Company’s common stock.\n",
      "A summary of the amounts related to option exercises, is as follows:\n",
      "Year Ended December 31,\n",
      "2023 2022 2021\n",
      "(in thousands)\n",
      "Total intrinsic value of options exercised $ 610,594  $ 345,839  $ 1,362,599 \n",
      "Cash received from options exercised 169,990  35,746  174,414 \n",
      "59\n",
      "[['Dividend yield', '', '—', None, '%', '', '—', None, '%', '—', None, '%'], ['Expected volatility', '', '40% - 46%', None, None, '', '38% - 52%', None, None, '34% - 41%', None, None], ['Risk-free interest rate', '', '3.57% - 4.56%', None, None, '', '1.71% - 3.79%', None, None, '1.08% - 1.62%', None, None], ['Suboptimal exercise factor', '', '4.22 - 4.30', None, None, '', '4.71 - 4.82', None, None, '3.81 - 3.98', None, None], ['Valuation data:', '', '', None, None, '', '', None, None, '', None, None], ['Weighted-average fair value (per share)', '', '$', '211.27', '', '', '$', '155.88', '', '$', '259.01', ''], ['Total stock-based compensation expense (in thousands)', '', '339,368', None, '', '', '575,452', None, '', '403,220', None, ''], ['Total income tax impact on provision (in thousands)', '', None, None, None, '', '127,289', None, '', '89,642', None, '']]\n",
      "----\n",
      "Table of Contents\n",
      "Stock-Based Compensation\n",
      "Stock options are generally vested in full upon grant date and exercisable for the full ten year contractual term regardless of employment status. Stock\n",
      "options granted to certain named executive officers vest on the one-year anniversary of the grant date, subject to the employee’s continuous employment or\n",
      "service with the Company through the vesting date. The following table summarizes the assumptions used to value option grants using the lattice-binomial\n",
      "model and the valuation data:\n",
      " \n",
      "  Year Ended December 31,\n",
      "  2023 2022 2021\n",
      "Dividend yield — % — % — %\n",
      "Expected volatility 40% - 46% 38% - 52% 34% - 41%\n",
      "Risk-free interest rate 3.57% - 4.56% 1.71% - 3.79% 1.08% - 1.62%\n",
      "Suboptimal exercise factor 4.22 - 4.30 4.71 - 4.82 3.81 - 3.98\n",
      "Valuation data:\n",
      "Weighted-average fair value (per share) $ 211.27  $ 155.88  $ 259.01 \n",
      "Total stock-based compensation expense (in thousands) 339,368  575,452  403,220 \n",
      "Total income tax impact on provision (in thousands) 61,588  127,289  89,642 \n",
      "The Company considers several factors in determining the suboptimal exercise factor, including the historical and estimated option exercise behavior.\n",
      "The Company calculates expected volatility based solely on implied volatility. The Company believes that implied volatility of publicly traded options in\n",
      "its common stock is more reflective of market conditions, and given consistently high trade volumes of the options, can reasonably be expected to be a better\n",
      "indicator of expected volatility than historical volatility of its common stock.\n",
      "In valuing shares issued under the Company’s employee stock option plans, the Company bases the risk-free interest rate on U.S. Treasury zero-coupon\n",
      "issues with terms similar to the contractual term of the options. The Company does not anticipate paying any cash dividends in the foreseeable future and\n",
      "therefore uses an expected dividend yield of zero in the option valuation model. The Company does not use a post-vesting termination rate as options are\n",
      "generally fully vested upon grant date.\n",
      "The total fair value of stock options that vested during the year ended December 31, 2023 was $311 million. The Company did not grant any stock\n",
      "options subject to vesting conditions in the years ended December 31, 2022 and 2021. As of December 31, 2023, $26 million of total unrecognized\n",
      "compensation cost related to nonvested stock options is expected to be recognized over a weighted-average period of 0.45 years.\n",
      "Stock Repurchases\n",
      "In March 2021, the Company’s Board of Directors authorized the repurchase of up to $5 billion of its common stock, with no expiration date, and in\n",
      "September 2023, the Board of Directors increased the share repurchase authorization by an additional $10 billion, also with no expiration date. Stock\n",
      "repurchases may be effected through open market repurchases in compliance with Rule 10b-18 under the Exchange Act, including through the use of trading\n",
      "plans intended to qualify under Rule 10b5-1 under the Exchange Act, privately-negotiated transactions, accelerated stock repurchase plans, block purchases, or\n",
      "other similar purchase techniques and in such amounts as management deems appropriate. The Company is not obligated to repurchase any specific number of\n",
      "shares, and the timing and actual number of shares repurchased will depend on a variety of factors, including the Company’s stock price, general economic,\n",
      "business and market conditions, and alternative investment opportunities. The Company may discontinue any repurchases of its common stock at any time\n",
      "without prior notice. During the year ended December 31, 2023, the Company repurchased 14,513,790 shares for an aggregate amount of $6,045 million. As of\n",
      "December 31, 2023, $8.4 billion remains available for repurchases. Shares repurchased by the Company are accounted for when the transaction is settled.\n",
      "Direct costs incurred to acquire the shares are included in the total cost of the shares.\n",
      "Accumulated Other Comprehensive Income (Loss)\n",
      "The following table summarizes the changes in accumulated balances of other comprehensive income (loss), net of tax:\n",
      "60\n",
      "[['Current tax provision:', '', None, '', None, '', '', None], ['Federal', '$', '854,170', '$', '109,910', '', '$', '57,526'], ['State', '181,684', None, '119,795', None, '', '109,641', None], ['Foreign', '304,539', None, '676,827', None, '', '357,189', None], ['Total current', '1,340,393', None, '906,532', None, '', '524,356', None], ['Deferred tax provision:', '', None, '', None, '', '', None], ['Federal', '(412,760)', None, '(52,434)', None, '', '188,937', None], ['State', '(55,475)', None, '(30,691)', None, '', '(2,700)', None], ['Foreign', '(74,743)', None, '(51,402)', None, '', '13,282', None], ['Total deferred', '(542,978)', None, '(134,527)', None, '', '199,519', None], ['Provision for income taxes', '$', '797,415', '$', '772,005', '', '$', '723,875']]\n",
      "----\n",
      "Table of Contents\n",
      "Foreign Currency Change in Unrealized\n",
      "Translation Gains (Losses) on Cash\n",
      "Adjustments Flow Hedges Total\n",
      "(in thousands)\n",
      "Balances as of December 31, 2020 $ 44,398  $ —  $ 44,398 \n",
      "Other comprehensive income (loss) before reclassifications (84,893) —  (84,893)\n",
      "Balances as of December 31, 2021 (40,495) —  (40,495)\n",
      "Other comprehensive income (loss) before reclassifications (176,811) —  (176,811)\n",
      "Balances as of December 31, 2022 (217,306) —  (217,306)\n",
      "Other comprehensive income (loss) before reclassifications 113,384  (120,023) (6,639)\n",
      "Balances as of December 31, 2023 $ (103,922) $ (120,023) $ (223,945)\n",
      "10.    Income Taxes\n",
      "Income before provision for income taxes was as follows:\n",
      "  Year Ended December 31,\n",
      "  2023 2022 2021\n",
      "  (in thousands)\n",
      "United States $ 5,602,762  $ 4,623,218  $ 5,349,749 \n",
      "Foreign 602,643  640,711  490,354 \n",
      "Income before income taxes $ 6,205,405  $ 5,263,929  $ 5,840,103 \n",
      "The components of provision for income taxes for all periods presented were as follows:\n",
      " \n",
      "  Year Ended December 31,\n",
      "  2023 2022 2021\n",
      "  (in thousands)\n",
      "Current tax provision:\n",
      "Federal $ 854,170  $ 109,910  $ 57,526 \n",
      "State 181,684  119,795  109,641 \n",
      "Foreign 304,539  676,827  357,189 \n",
      "Total current 1,340,393  906,532  524,356 \n",
      "Deferred tax provision:\n",
      "Federal (412,760) (52,434) 188,937 \n",
      "State (55,475) (30,691) (2,700)\n",
      "Foreign (74,743) (51,402) 13,282 \n",
      "Total deferred (542,978) (134,527) 199,519 \n",
      "Provision for income taxes $ 797,415  $ 772,005  $ 723,875 \n",
      "A reconciliation of the provision for income taxes to the amount computed by applying the 21% statutory Federal income tax rate to income before\n",
      "income taxes is as follows:\n",
      "61\n",
      "[['Deferred tax assets:', '', None, '', '', None], ['Stock-based compensation', '$', '486,876', '', '$', '443,456'], ['Tax credits and net operating loss carryforwards', '544,431', None, '', '409,411', None], ['Capitalized research expenses', '593,439', None, '', '323,998', None], ['Accruals and reserves', '137,251', None, '', '119,732', None], ['Operating lease liabilities', '516,574', None, '', '551,418', None], ['Unrealized losses', '62,213', None, '', '—', None], ['Other', '11,615', None, '', '2,234', None], ['Total deferred tax assets', '2,352,399', None, '', '1,850,249', None], ['Valuation allowance', '(442,293)', None, '', '(343,342)', None], ['Net deferred tax assets', '1,910,106', None, '', '1,506,907', None], ['Deferred tax liabilities:', '', None, '', '', None], ['Depreciation & amortization', '(357,477)', None, '', '(456,717)', None], ['Operating right-of-use lease assets', '(435,216)', None, '', '(473,928)', None], ['Unrealized gains', '—', None, '', '(47,283)', None], ['Acquired intangibles', '(233,433)', None, '', '(267,438)', None], ['Other', '(9,430)', None, '', '—', None], ['Total deferred tax liabilities', '(1,035,556)', None, '', '(1,245,366)', None], ['Net deferred tax assets', '$', '874,550', '', '$', '261,541']]\n",
      "----\n",
      "Table of Contents\n",
      "  Year Ended December 31,\n",
      "  2023 2022 2021\n",
      "  (in thousands)\n",
      "Expected tax expense at U.S. Federal statutory tax rate $ 1,303,123  $ 1,105,428  $ 1,226,422 \n",
      "State income taxes, net of Federal income tax effect 104,717  92,084  111,400 \n",
      "Foreign earnings at other than U.S. rates (32,292) 104,665  (86,489)\n",
      "Research and development tax credit (87,036) (146,615) (82,909)\n",
      "Excess tax benefits on stock-based compensation (119,043) (75,211) (290,899)\n",
      "Foreign-derived intangible income deduction (426,597) (361,013) (192,238)\n",
      "Nontaxable and nondeductible items 41,782  44,046  37,144 \n",
      "Other 12,761  8,621  1,444 \n",
      "Provision for income taxes $ 797,415  $ 772,005  $ 723,875 \n",
      "Effective Tax Rate 13 % 15 % 12 %\n",
      "The components of deferred tax assets and liabilities were as follows:\n",
      " \n",
      "  As of December 31,\n",
      "  2023 2022\n",
      "  (in thousands)\n",
      "Deferred tax assets:\n",
      "Stock-based compensation $ 486,876  $ 443,456 \n",
      "Tax credits and net operating loss carryforwards 544,431  409,411 \n",
      "Capitalized research expenses 593,439  323,998 \n",
      "Accruals and reserves 137,251  119,732 \n",
      "Operating lease liabilities 516,574  551,418 \n",
      "Unrealized losses 62,213  — \n",
      "Other 11,615  2,234 \n",
      "Total deferred tax assets 2,352,399  1,850,249 \n",
      "Valuation allowance (442,293) (343,342)\n",
      "Net deferred tax assets 1,910,106  1,506,907 \n",
      "Deferred tax liabilities:\n",
      "Depreciation & amortization (357,477) (456,717)\n",
      "Operating right-of-use lease assets (435,216) (473,928)\n",
      "Unrealized gains —  (47,283)\n",
      "       Acquired intangibles (233,433) (267,438)\n",
      "       Other (9,430) — \n",
      "Total deferred tax liabilities (1,035,556) (1,245,366)\n",
      "Net deferred tax assets $ 874,550  $ 261,541 \n",
      "The following table shows the deferred tax assets and liabilities within our Consolidated Balance Sheets:\n",
      "  As of December 31,\n",
      "  2023 2022\n",
      "  (in thousands)\n",
      "Total deferred tax assets:\n",
      "Other non-current assets $ 1,000,760  $ 261,541 \n",
      "Total deferred tax liabilities:\n",
      "Other non-current liabilities (126,210) — \n",
      "Net deferred tax assets $ 874,550  $ 261,541 \n",
      "62\n",
      "[['Balance at the beginning of the year', '$', '226,977', '$', '202,557', '$', '140,124'], ['Increases related to tax positions taken during the current period', '65,630', None, '26,865', None, '35,317', None], ['Increases related to tax positions taken during prior periods', '76,794', None, '—', None, '27,116', None], ['Decreases related to tax positions taken during prior periods', '(10,117)', None, '(2,445)', None, '—', None], ['Decreases related to settlements with taxing authorities', '(32,179)', None, '—', None, '—', None], ['Decreases related to expiration of statute of limitations', '—', None, '—', None, '—', None], ['Balance at the end of the year', '$', '327,105', '$', '226,977', '$', '202,557']]\n",
      "----\n",
      "Table of Contents\n",
      "As of December 31, 2023, for tax return purposes, the Company had $582 million of California R&D tax credit carryforwards which can be carried\n",
      "forward indefinitely, $838 million of state net operating loss carryforwards which will begin to expire in 2026, $19 million of foreign tax credit carryforwards\n",
      "which will begin to expire in 2033, and $421 million of foreign net operating loss carryforwards which will begin to expire in 2024.\n",
      "In evaluating its ability to realize the net deferred tax assets, the Company considered all available positive and negative evidence, including its past\n",
      "operating results and the forecast of future market growth, forecasted earnings, future taxable income, and prudent and feasible tax planning strategies. As of\n",
      "December 31, 2023, the valuation allowance of $442 million was primarily related to California R&D tax credits, state net operating loss carryforwards, and\n",
      "foreign tax credits that the Company does not expect to realize.\n",
      "At December 31, 2023, we have not provided for applicable U.S. income and foreign withholding taxes on approximately $52 million of our foreign\n",
      "undistributed earnings because such earnings are intended to be indefinitely reinvested. At December 31, 2023, we provided taxes and recorded a deferred tax\n",
      "liability on our undistributed foreign earnings for which we are not indefinitely reinvested.\n",
      "The unrecognized tax benefits that are not expected to result in payment or receipt of cash within one year are classified as “Other non-current liabilities”\n",
      "and a reduction of deferred tax assets which is classified as \"Other non-current assets\" in the Consolidated Balance Sheets. As of December 31, 2023 and 2022,\n",
      "the total amount of gross unrecognized tax benefits was $327 million and $227 million, respectively, of which $188 million and $155 million, respectively, if\n",
      "recognized, would favorably impact the Company’s effective tax rate. The aggregate changes in the Company’s total gross amount of unrecognized tax benefits\n",
      "are summarized as follows:\n",
      " \n",
      "As of December 31,\n",
      "2023 2022 2021\n",
      "(in thousands)\n",
      "Balance at the beginning of the year $ 226,977 $ 202,557 $ 140,124 \n",
      "Increases related to tax positions taken during the current period 65,630  26,865  35,317 \n",
      "Increases related to tax positions taken during prior periods 76,794  —  27,116 \n",
      "Decreases related to tax positions taken during prior periods (10,117) (2,445) — \n",
      "Decreases related to settlements with taxing authorities (32,179) —  — \n",
      "Decreases related to expiration of statute of limitations —  —  — \n",
      "Balance at the end of the year $ 327,105 $ 226,977 $ 202,557 \n",
      "The Company includes interest and penalties related to unrecognized tax benefits within the provision for income taxes and in “Other non-current\n",
      "liabilities” in the Consolidated Balance Sheets. During the years ended December 31, 2023, 2022 and 2021, the Company recorded $25 million, $2 million, and\n",
      "less than $1 million, respectively, of interest and penalties in the provision for income taxes. The amount of interest and penalties accrued at December 31,\n",
      "2023 and 2022 was $28 million and $3 million, respectively.\n",
      "The Company files U.S. Federal, state and foreign tax returns. The Company is currently under examination by the IRS for years 2016 through 2018 and\n",
      "is subject to examination for 2019 through 2022. The foreign and state tax returns for years 2016 through 2022 are subject to examination by various state and\n",
      "foreign jurisdictions. While the Company is in various stages of inquiry and examination with certain taxing authorities and we believe that our tax positions\n",
      "will more likely than not be sustained, it is nonetheless possible that future obligations related to these matters could arise. We believe that adequate amounts\n",
      "have been reserved for any adjustments that may ultimately result from an examination.\n",
      "Given the potential outcome of current examinations, it is reasonably possible that the balance of unrecognized tax benefits could significantly change\n",
      "within the next twelve months. However, an estimate of the range of reasonably possible adjustments cannot be made at this time.\n",
      "11.    Employee Benefit Plan\n",
      "The Company maintains a 401(k) savings plan covering substantially all of its employees. Eligible employees may contribute up to 80% of their annual\n",
      "salary through payroll deductions, but not more than the statutory limits set by the Internal Revenue Service. The Company matches employee contributions at\n",
      "the discretion of the Board. During the years ended December 31, 2023, 2022 and 2021, the Company’s matching contributions totaled $114 million, $102\n",
      "million and $85 million, respectively.\n",
      "63\n",
      "[['Pension benefits', '$', '57,285', '', '$', '127,885', '', '$', '111,133'], ['Health benefits', '85,157', None, '', '96,285', None, '', '83,153', None], ['Total contributions', '$', '142,442', '', '$', '224,170', '', '$', '194,286']]\n",
      "----\n",
      "Table of Contents\n",
      "Multiemployer Benefit Plans\n",
      "The Company contributes to various multiemployer defined pension plans under the terms of collective bargaining agreements that cover our union-\n",
      "represented employees. The risks of participating in multiemployer pension plans are different from single-employer plans such that (i) contributions made by\n",
      "the Company to the multiemployer pension plans may be used to provide benefits to employees of other participating employers; (ii) if the Company chooses\n",
      "to stop participating in the multiemployer pension plans, it may be required to pay those plans an amount based on the underfunded status of the plan; and (iii)\n",
      "if a company stops contributing to the multiemployer pension plan, the unfunded obligations of the plan may become the obligation of the remaining\n",
      "participating employers. The Company also contributes to various other multiemployer benefit plans that provide health and welfare benefits to both active and\n",
      "retired participants. The Company does not participate in any multiemployer benefit plans that are individually significant to the Company.\n",
      "The following table summarizes the Company's contributions to multiemployer pension and health plans for the years ended December 31, 2023, 2022\n",
      "and 2021, respectively:\n",
      "  Year Ended December 31,\n",
      "  2023 2022 2021\n",
      "  (in thousands)\n",
      "Pension benefits $ 57,285  $ 127,885  $ 111,133 \n",
      "Health benefits 85,157  96,285  83,153 \n",
      "Total contributions $ 142,442  $ 224,170  $ 194,286 \n",
      "12.    Segment and Geographic Information\n",
      "The Company operates as one operating segment. The Company's chief operating decision maker (\"CODM\") is its co-chief executive officers, who\n",
      "review financial information presented on a consolidated basis for purposes of making operating decisions, assessing financial performance and allocating\n",
      "resources.\n",
      "    Total U.S. revenues were $13.8 billion, $13.0 billion and $12.1 billion for the years ended December 31, 2023, 2022 and 2021, respectively. See Note 2\n",
      "Revenue Recognition for additional information about streaming revenue by region.\n",
      "    The Company's long-lived tangible assets, as well as the Company's operating lease right-of-use assets recognized on the Consolidated Balance Sheets were\n",
      "located as follows:\n",
      "As of December 31,\n",
      "2023 2022\n",
      "(in thousands)\n",
      "United States $ 2,724,710  $ 2,745,071 \n",
      "International 843,633  880,308 \n",
      "64\n",
      "[['Netflix Entretenimento Brasil LTDA', '', 'Brazil', '', '100', '%'], ['Netflix International B.V.', '', 'The Netherlands', '', '100', '%'], ['Netflix G.K.', '', 'Japan', '', '100', '%'], ['Netflix Studios, LLC', '', 'United States', '', '100', '%'], ['Netflix Global, LLC', '', 'United States', '', '100', '%'], ['Netflix Media, LLC', '', 'United States', '', '100', '%'], ['Netflix México S. de R.L. de C.V.', '', 'Mexico', '', '100', '%'], ['Netflix Pte. Ltd.', '', 'Singapore', '', '100', '%'], ['Netflix Services France S.A.S.', '', 'France', '', '100', '%'], ['Netflix Services UK Limited', '', 'United Kingdom', '', '100', '%'], ['Netflix Australia Pty Ltd', '', 'Australia', '', '100', '%'], ['Netflix Services Germany GmbH', '', 'Germany', '', '100', '%'], ['Netflix Services Italy S.R.L.', '', 'Italy', '', '100', '%'], ['Netflix Worldwide Entertainment, LLC', '', 'United States', '', '100', '%'], ['Netflix Services Canada ULC', '', 'Canada', '', '100', '%'], ['Netflix Servicios de Transmisión España, S.L.', '', 'Spain', '', '100', '%']]\n",
      "----\n",
      "EXHIBIT 21.1\n",
      "NETFLIX, INC.\n",
      "LIST OF SIGNIFICANT SUBSIDIARIES*\n",
      "Legal Name Jurisdiction Percent Owned\n",
      "Netflix Entretenimento Brasil LTDA Brazil 100 %\n",
      "Netflix International B.V. The Netherlands 100 %\n",
      "Netflix G.K. Japan 100 %\n",
      "Netflix Studios, LLC United States 100 %\n",
      "Netflix Global, LLC United States 100 %\n",
      "Netflix Media, LLC United States 100 %\n",
      "Netflix México S. de R.L. de C.V. Mexico 100 %\n",
      "Netflix Pte. Ltd. Singapore 100 %\n",
      "Netflix Services France S.A.S. France 100 %\n",
      "Netflix Services UK Limited United Kingdom 100 %\n",
      "Netflix Australia Pty Ltd Australia 100 %\n",
      "Netflix Services Germany GmbH Germany 100 %\n",
      "Netflix Services Italy S.R.L. Italy 100 %\n",
      "Netflix Worldwide Entertainment, LLC United States 100 %\n",
      "Netflix Services Canada ULC Canada 100 %\n",
      "Netflix Servicios de Transmisión España, S.L. Spain 100 %\n",
      "* Pursuant to Item 601(b)(21)(ii) of Regulation S-K, the names of other subsidiaries of Netflix Inc. are omitted\n",
      "because, considered in the aggregate, they would not constitute a significant subsidiary as of the end of the\n",
      "year covered by this report.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "pdf = pdfplumber.open(\"report.pdf\")\n",
    "for i in range(len(pdf.pages)):\n",
    "    page = pdf.pages[i]\n",
    "    table = page.extract_table()\n",
    "    if table is not None:\n",
    "        text = page.extract_text(keep_blank_chars=True)\n",
    "        print(table)\n",
    "        print(\"----\")\n",
    "        print(text)\n",
    "\n",
    "\"\"\"\n",
    "AW NOTE:\n",
    "- pdfplumber可以提取表格和文本，无论表格是img还是文本都可以被识别\n",
    "- 提取出来的table为list，可以直接被pandas读取\n",
    "- 开源免费\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<pdfplumber.display.PageImage at 0x26c79614cb0>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0 = pdf.pages[43]\n",
    "im = p0.to_image()\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<pdfplumber.display.PageImage at 0x26c79614cb0>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.reset().debug_tablefinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of Contents\n",
      "NETFLIX, INC.\n",
      "CONSOLIDATED STATEMENTS OF CASH FLOWS\n",
      "(in thousands)\n",
      "   Year Ended December 31,\n",
      "   2023 2022 2021\n",
      "Cash flows from operating activities:\n",
      "Net income $ 5,407,990  $ 4,491,924  $ 5,116,228 \n",
      "Adjustments to reconcile net income to net cash provided by operating activities:\n",
      "Additions to content assets (12,554,703) (16,839,038) (17,702,202)\n",
      "Change in content liabilities (585,602) 179,310  232,898 \n",
      "Amortization of content assets 14,197,437  14,026,132  12,230,367 \n",
      "Depreciation and amortization of property, equipment and intangibles 356,947  336,682  208,412 \n",
      "Stock-based compensation expense 339,368  575,452  403,220 \n",
      "Foreign currency remeasurement loss (gain) on debt 176,296  (353,111) (430,661)\n",
      "Other non-cash items 512,075  533,543  376,777 \n",
      "Deferred income taxes (459,359) (166,550) 199,548 \n",
      "Changes in operating assets and liabilities:\n",
      "Other current assets (181,003) (353,834) (369,681)\n",
      "Accounts payable 93,502  (158,543) 145,115 \n",
      "Accrued expenses and other liabilities 103,565  (55,513) 180,338 \n",
      "Deferred revenue 178,708  27,356  91,350 \n",
      "Other non-current assets and liabilities (310,920) (217,553) (289,099)\n",
      "Net cash provided by operating activities 7,274,301  2,026,257  392,610 \n",
      "Cash flows from investing activities:\n",
      "Purchases of property and equipment (348,552) (407,729) (524,585)\n",
      "Change in other assets —  —  (26,919)\n",
      "Acquisitions —  (757,387) (788,349)\n",
      "Purchases of short-term investments (504,862) (911,276) — \n",
      "Proceeds from maturities of short-term investments 1,395,165  —  — \n",
      "Net cash provided by (used in) investing activities 541,751  (2,076,392) (1,339,853)\n",
      "Cash flows from financing activities:\n",
      "Repayments of debt —  (700,000) (500,000)\n",
      "Proceeds from issuance of common stock 169,990  35,746  174,414 \n",
      "Repurchases of common stock (6,045,347) —  (600,022)\n",
      "Taxes paid related to net share settlement of equity awards —  —  (224,168)\n",
      "Other financing activities (75,446) —  — \n",
      "Net cash used in financing activities (5,950,803) (664,254) (1,149,776)\n",
      "Effect of exchange rate changes on cash, cash equivalents and restricted cash 82,684  (170,140) (86,740)\n",
      "Net increase (decrease) in cash, cash equivalents and restricted cash 1,947,933  (884,529) (2,183,759)\n",
      "Cash, cash equivalents and restricted cash, beginning of year 5,170,582  6,055,111  8,238,870 \n",
      "Cash, cash equivalents and restricted cash, end of year $ 7,118,515  $ 5,170,582  $ 6,055,111 \n",
      "Supplemental disclosure:\n",
      "Income taxes paid $ 1,154,973  $ 811,720  $ 509,265 \n",
      "Interest paid 684,504  701,693  763,432 \n",
      "See accompanying notes to consolidated financial statements.\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "text = p0.extract_text(keep_blank_chars=True,)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval with fixed size chunking - pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\long8\\AppData\\Local\\Temp\\ipykernel_22764\\2384133918.py:12: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding_model = HuggingFaceEmbeddings()\n",
      "Created a chunk of size 323, which is longer than the specified 256\n",
      "Created a chunk of size 294, which is longer than the specified 256\n",
      "Created a chunk of size 1249, which is longer than the specified 256\n",
      "Created a chunk of size 928, which is longer than the specified 256\n",
      "Created a chunk of size 1791, which is longer than the specified 256\n",
      "Created a chunk of size 1754, which is longer than the specified 256\n",
      "Created a chunk of size 483, which is longer than the specified 256\n",
      "Created a chunk of size 660, which is longer than the specified 256\n",
      "Created a chunk of size 652, which is longer than the specified 256\n",
      "Created a chunk of size 540, which is longer than the specified 256\n",
      "Created a chunk of size 899, which is longer than the specified 256\n",
      "Created a chunk of size 398, which is longer than the specified 256\n",
      "Created a chunk of size 298, which is longer than the specified 256\n",
      "Created a chunk of size 413, which is longer than the specified 256\n",
      "Created a chunk of size 1200, which is longer than the specified 256\n",
      "Created a chunk of size 272, which is longer than the specified 256\n",
      "Created a chunk of size 304, which is longer than the specified 256\n",
      "Created a chunk of size 502, which is longer than the specified 256\n",
      "Created a chunk of size 271, which is longer than the specified 256\n",
      "Created a chunk of size 609, which is longer than the specified 256\n",
      "Created a chunk of size 563, which is longer than the specified 256\n",
      "Created a chunk of size 692, which is longer than the specified 256\n",
      "Created a chunk of size 1460, which is longer than the specified 256\n",
      "Created a chunk of size 1547, which is longer than the specified 256\n",
      "Created a chunk of size 569, which is longer than the specified 256\n",
      "Created a chunk of size 381, which is longer than the specified 256\n",
      "Created a chunk of size 1222, which is longer than the specified 256\n",
      "Created a chunk of size 301, which is longer than the specified 256\n",
      "Created a chunk of size 1589, which is longer than the specified 256\n",
      "Created a chunk of size 314, which is longer than the specified 256\n",
      "Created a chunk of size 2525, which is longer than the specified 256\n",
      "Created a chunk of size 926, which is longer than the specified 256\n",
      "Created a chunk of size 1037, which is longer than the specified 256\n",
      "Created a chunk of size 1370, which is longer than the specified 256\n",
      "Created a chunk of size 532, which is longer than the specified 256\n",
      "Created a chunk of size 1050, which is longer than the specified 256\n",
      "Created a chunk of size 590, which is longer than the specified 256\n",
      "Created a chunk of size 322, which is longer than the specified 256\n",
      "Created a chunk of size 292, which is longer than the specified 256\n",
      "Created a chunk of size 292, which is longer than the specified 256\n",
      "Created a chunk of size 291, which is longer than the specified 256\n",
      "Created a chunk of size 363, which is longer than the specified 256\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# Initialize OpenAI models\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "# embedding_model = OpenAIEmbeddings()\n",
    "# Load and preprocess documents\n",
    "\n",
    "# Split documents into chunks for vector storage\n",
    "text_splitter = CharacterTextSplitter(chunk_size=256, chunk_overlap=20)\n",
    "docs = text_splitter.create_documents(documents_naive)\n",
    "\n",
    "# Create a vector database using FAISS\n",
    "vector_db = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Create a conversation chain with retrieval capabilities\n",
    "retrieval_chain = ConversationalRetrievalChain.from_llm(llm, vector_db.as_retriever())\n",
    "\n",
    "\n",
    "# AW NOTE: 使用LLM进行chat，使用HuggingFaceEmbeddings进行文本embedding，使用FAISS进行文本检索。提示词中可以明确要求LLM不使用existing的知识，并且如果不知道就回答不知道。手动设置prompt需要倒入额外的模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The BLEU score for the Transformer (base model) on the English-to-German '\n",
      " 'translation is 27.3, and for the English-to-French translation, it is 38.1.')\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"question\": \"What is the score for Transformer (base model)\", \"chat_history\": []})\n",
    "pprint(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The English constituency parsing results for Zhu et al. (2013) are 90.4 F1 '\n",
      " 'when using WSJ only in a discriminative setting, and 91.3 F1 when using a '\n",
      " 'semi-supervised approach.')\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"question\": \"What is English constituency parsing Results for Zhu et al. (2013)\", \"chat_history\": []})\n",
    "pprint(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval with page chunking - llama parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\long8\\AppData\\Local\\Temp\\ipykernel_22764\\1319817094.py:14: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding_model = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain.docstore.document import Document\n",
    "from pprint import pprint\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create a vector database using FAISS\n",
    "docs = [Document(page_content=documents[i].text,metadata=documents[i].metadata) for i in range(len(documents))]\n",
    "vector_db = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Create a conversation chain with retrieval capabilities\n",
    "retrieval_chain_llama = ConversationalRetrievalChain.from_llm(llm, vector_db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The BLEU score for the Transformer (base model) on the English-to-German '\n",
      " 'translation task is 27.3, and on the English-to-French translation task, it '\n",
      " 'is 38.1.')\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain_llama.invoke({\"question\": \"What is the score for Transformer (base model)\", \"chat_history\": []})\n",
    "pprint(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The English constituency parsing results for Zhu et al. (2013) are as '\n",
      " 'follows:\\n'\n",
      " '\\n'\n",
      " '- When using WSJ only, discriminative training, the WSJ 23 F1 score is '\n",
      " '90.4.\\n'\n",
      " '- When using semi-supervised training, the WSJ 23 F1 score is 91.3.')\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain_llama.invoke({\"question\": \"What is English constituency parsing Results for Zhu et al. (2013)\", \"chat_history\": []})\n",
    "pprint(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval with markdown - marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\long8\\AppData\\Local\\Temp\\ipykernel_22764\\506807065.py:14: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding_model = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain.docstore.document import Document\n",
    "from pprint import pprint\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "# Create a vector database using FAISS\n",
    "splited_text = text.split(\"####\")\n",
    "docs = [Document(page_content=splited_text[i],metadata={\"title\":rendered.metadata['table_of_contents'][i]['title']}) for i in range(len(splited_text))]\n",
    "vector_db = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Create a conversation chain with retrieval capabilities\n",
    "retrieval_chain_maker = ConversationalRetrievalChain.from_llm(llm, vector_db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Transformer (base model) achieves a BLEU score of 25.8 on the '\n",
      " 'English-to-German translation task development set, newstest2013.')\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain_maker.invoke({\"question\": \"What is the score for Transformer (base model)\", \"chat_history\": []})\n",
    "pprint(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('For English constituency parsing, Zhu et al. (2013) achieved an F1 score of '\n",
      " '90.4 in the WSJ-only, discriminative setting, and an F1 score of 91.3 in the '\n",
      " 'semi-supervised setting, according to the results presented.')\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain_maker.invoke({\"question\": \"What is English constituency parsing Results for Zhu et al. (2013)\", \"chat_history\": []})\n",
    "pprint(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval with map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import (\n",
    "    StuffDocumentsChain, LLMChain, ReduceDocumentsChain\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# This controls how each document will be formatted. Specifically,\n",
    "# it will be passed to `format_document` - see that function for more\n",
    "# details.\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"],\n",
    "     template=\"{page_content}\"\n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "# The prompt here should take as an input variable the\n",
    "# `document_variable_name`\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize this content: {context}\"\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name\n",
    ")\n",
    "chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    ")\n",
    "# If we wanted to, we could also pass in collapse_documents_chain\n",
    "# which is specifically aimed at collapsing documents BEFORE\n",
    "# the final call.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Collapse this content: {context}\"\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "collapse_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name\n",
    ")\n",
    "chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    collapse_documents_chain=collapse_documents_chain,\n",
    ")\n",
    "\n",
    "# AW NOTE：这种方法将整个文档作为一个context，然后使用LLM进行summarize或者collapse。回答问题时，使用summary或者collapse后的文档作为context，然后回答问题。这种方法不是RAG，而是使用LLM进行文档处理。这种方法不适合处理数据，但是适合处理小说或者简单的文章。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The paper \"Attention Is All You Need\" introduces the Transformer model, '\n",
      " 'which utilizes attention mechanisms exclusively, discarding the need for '\n",
      " 'recurrence and convolution. This architecture enhances parallelization and '\n",
      " 'performance in tasks like machine translation. The Transformer achieved '\n",
      " 'notable BLEU scores on the WMT 2014 English-to-German and English-to-French '\n",
      " 'tasks, with lower training costs than previous models. \\n'\n",
      " '\\n'\n",
      " 'Key components of the model include:\\n'\n",
      " '\\n'\n",
      " '- **Multi-Head Attention**: This enables the model to process information '\n",
      " 'from different subspaces efficiently.\\n'\n",
      " '- **Attention Applications**: The model employs attention through '\n",
      " 'encoder-decoder attention, encoder self-attention, and decoder '\n",
      " 'self-attention with masking.\\n'\n",
      " '- **Feed-Forward Networks**: Each layer features a network with linear '\n",
      " 'transformations and ReLU activation.\\n'\n",
      " '- **Embeddings and Softmax**: Tokens are transformed into vectors with '\n",
      " 'shared weights between embedding layers and pre-softmax transformations.\\n'\n",
      " '- **Positional Encoding**: Sine and cosine functions provide sequence order '\n",
      " 'information due to the lack of recurrence.\\n'\n",
      " '\\n'\n",
      " 'The model demonstrates significant advantages over traditional recurrent and '\n",
      " 'convolutional layers, offering reduced path lengths and faster computation. '\n",
      " 'Training involved using byte-pair encoding and several regularization '\n",
      " 'techniques, with the Adam optimizer and a learning rate schedule.\\n'\n",
      " '\\n'\n",
      " 'The Transformer model outperformed previous state-of-the-art models in '\n",
      " 'translation tasks, achieving high BLEU scores with reduced training costs '\n",
      " 'and time. It also generalizes well to other tasks like English constituency '\n",
      " \"parsing, showing promise for applications beyond text. The architecture's \"\n",
      " 'reliance on attention mechanisms facilitates faster training and improved '\n",
      " 'performance, with the code available for public use.\\n'\n",
      " '\\n'\n",
      " 'Additionally, the content discusses references related to machine learning '\n",
      " 'and attention visualizations in neural networks, highlighting how attention '\n",
      " 'mechanisms manage dependencies and resolve linguistic relationships in '\n",
      " 'sentences.')\n"
     ]
    }
   ],
   "source": [
    "result = chain.run(question=\"What is English constituency parsing Results for Zhu et al. (2013)\", input_documents=docs)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The table outlines variations of the Transformer model, particularly for '\n",
      " 'English-to-German translation tasks. Key metrics include per-word piece '\n",
      " \"perplexities using byte-pair encoding and BLEU scores. Here's a summary of \"\n",
      " 'the findings:\\n'\n",
      " '\\n'\n",
      " \"- **Base Model:** Details the model's parameters, such as size and \"\n",
      " 'dimensions, alongside dropout, training steps, perplexity, and BLEU scores.\\n'\n",
      " '\\n'\n",
      " '- **Results Overview:**\\n'\n",
      " '  - **Row (A):** Changes in attention head numbers indicate that too few or '\n",
      " 'too many heads decrease translation quality, affecting BLEU scores.\\n'\n",
      " '  - **Row (B):** Smaller attention key sizes degrade performance, suggesting '\n",
      " 'a more advanced compatibility function might improve results.\\n'\n",
      " '  - **Rows (C) & (D):** Larger models yield better performance; dropout is '\n",
      " 'essential to prevent overfitting.\\n'\n",
      " '  - **Row (E):** Using learned embeddings instead of sinusoidal positional '\n",
      " 'encoding results in similar performance to the base model.\\n'\n",
      " '\\n'\n",
      " '- **Additional Insights:** The Transformer model also shows potential for '\n",
      " 'other tasks like English constituency parsing, which is challenging due to '\n",
      " 'structural constraints and longer outputs. A 4-layer Transformer was trained '\n",
      " 'on the Wall Street Journal dataset, with both a supervised and '\n",
      " 'semi-supervised approach using different vocabulary sizes. Minimal parameter '\n",
      " \"tuning was applied, maintaining consistency with the translation model's \"\n",
      " 'settings.')\n"
     ]
    }
   ],
   "source": [
    "result = chain.run(question=\"What is the score for Transformer (base model)\", input_documents=docs)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
