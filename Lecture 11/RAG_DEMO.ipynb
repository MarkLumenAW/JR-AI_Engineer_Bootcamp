{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG with Python (Langchain)\n",
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_Rjw0DOvOO6tfAotfKsG_g.png)\n",
    "![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2496e7c6fedd7ffaa043895c23a4089638b0c21b-3840x2160.png&w=3840&q=75)\n",
    "![](https://cohere.com/_next/image?url=https%3A%2F%2Flh7-us.googleusercontent.com%2FmY4nN_0I3bcslVlC-dlw8tWsMBqsA33ai2spUc4PSodgcQFr0hlLsazK4MVeAIvEqmp8yk6QgbnKW0MR5CfyibybSpW2A7aGd-UHE7V3XVX-gtQkN8gscwk8Q3gUK5EmXLAwjumTCqWpc-DuyPJNRF8&w=2048&q=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain langchain-openai langchain-community faiss-cpu rank_bm25 langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import BaseDocumentCompressor\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "import tqdm as tqdm\n",
    "from langchain.docstore.document import Document\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "from pydantic import BaseModel\n",
    "load_dotenv()\n",
    "logging.disable(level=logging.INFO)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### .env\n",
    "# OPENAI_API_KEY=\"your api key\"\n",
    "# COHERE_API_KEY=\"your api key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "loader = TextLoader(\"./paul_graham_essay.txt\")\n",
    "documents = loader.load()\n",
    "WHOLE_DCOUMENT = documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_document = PromptTemplate(\n",
    "    input_variables=[\"WHOLE_DCOUMENT\"], template=\"{WHOLE_DCOUMENT}\"\n",
    ")\n",
    "\n",
    "prompt_chunk = PromptTemplate(\n",
    "    input_variables=[\"CHUNK_CONTENT\"], template=\"Here is the chunk we want to situate within the whole document\\n\\n{CHUNK_CONTENT}\\n\\n\"\n",
    "    \"Please give a short succinct context to situate this chunk within the overall document for \"\n",
    "    \"the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, chunk_size=256):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=200)\n",
    "    doc_chunks = text_splitter.create_documents(text)\n",
    "    for i, doc in enumerate(doc_chunks):\n",
    "        doc.metadata = {\"doc_id\":f\"doc_{i}\"}\n",
    "    return doc_chunks\n",
    "\n",
    "def create_embedding_retriever(documents_):\n",
    "    faiss_index = FAISS.from_documents(documents_, embedding=embeddings)\n",
    "    return faiss_index.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "def create_bm25_retriever(documents_):\n",
    "    bm25 = BM25Retriever.from_documents(documents_,language=\"en\")\n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBM25RerankerRetriever:\n",
    "    def __init__(self,\n",
    "                 embedding_retriever: BaseRetriever,\n",
    "                 bm25_retriever: BaseRetriever,\n",
    "                 reranker: CohereRerank):\n",
    "\n",
    "        self.embedding_retriever = embedding_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        self.reranker = reranker\n",
    "\n",
    "    def invoke(self, query):\n",
    "        vector_docs = self.embedding_retriever.invoke(query)\n",
    "        bm25_docs = self.bm25_retriever.invoke(query)\n",
    "\n",
    "        combined_docs = vector_docs + [\n",
    "            doc for doc in bm25_docs if doc not in vector_docs\n",
    "        ]\n",
    "\n",
    "        reranked_docs = self.reranker.compress_documents(combined_docs, query)\n",
    "\n",
    "        return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none contextual retriever\n",
    "chunks = split_text(documents[0].page_content)\n",
    "embedding_retriever = create_embedding_retriever(chunks)\n",
    "bm25_retriever = create_bm25_retriever(chunks)\n",
    "reranker= CohereRerank(top_n=3,model='rerank-english-v2.0')\n",
    "\n",
    "embedding_bm25_retriever_rerank = EmbeddingBM25RerankerRetriever(\n",
    "    embedding_retriever=embedding_retriever,\n",
    "    bm25_retriever=bm25_retriever,\n",
    "    reranker=reranker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1004/1004 [23:36<00:00,  1.41s/it] \n"
     ]
    }
   ],
   "source": [
    "# contextual retriever\n",
    "def create_contextual_chunks(chunks_):\n",
    "    contextual_chunks = []\n",
    "    for chunk in tqdm.tqdm(chunks_):\n",
    "        context = prompt_document.format(WHOLE_DCOUMENT=WHOLE_DCOUMENT)\n",
    "        chunk_content = prompt_chunk.format(CHUNK_CONTENT=chunk)\n",
    "        llm_response = llm.invoke(context + chunk_content).content\n",
    "        page_content = f\"\"\"Text: {chunk.page_content}\\nContext: {llm_response}\"\"\"\n",
    "        doc = Document(page_content=page_content, metadata=chunk.metadata)\n",
    "        contextual_chunks.append(doc)\n",
    "    return contextual_chunks\n",
    "\n",
    "contextual_chunks = create_contextual_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_embedding_retriever = create_embedding_retriever(contextual_chunks)\n",
    "contextual_bm25_retriever = create_bm25_retriever(contextual_chunks)\n",
    "embedding_bm25_retriever_rerank = EmbeddingBM25RerankerRetriever(\n",
    "    embedding_retriever=contextual_embedding_retriever,\n",
    "    bm25_retriever=contextual_bm25_retriever,\n",
    "    reranker=reranker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate question-context paris\n",
    "#123456789\n",
    "#123\n",
    "#456\n",
    "#789\n",
    "# llm - prompt\n",
    "# given the {#123} generate question and answer pairs \n",
    "# test dataset\n",
    "\n",
    "DEFAULT_QA_GENERATE_PROMPT_TMPL = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "[Context]\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Given the context information and no prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "Your task is to setup {num_questions_per_chunk} questions based on the [Context]. \n",
    "The questions should be diverse in nature \\\n",
    "across the document, and questions must be relevant to the [Context]. \n",
    "Restrict the questions to the [Context] information provided above. Do not fabricate questions outside of the [Context]\"\n",
    "\"\"\"\n",
    "\n",
    "class QuestionContextEvalDataset(BaseModel):\n",
    "    queries: Dict[str, str]\n",
    "    corpus: Dict[str, str]\n",
    "    relevant_docs: Dict[str, List[str]]\n",
    "    mode: str = \"text\"\n",
    "\n",
    "    @property\n",
    "    def query_docid_pairs(self) -> List[Tuple[str, str]]:\n",
    "        return [\n",
    "            (query,self.relevant_docs[query_id])\n",
    "            for query_id, query in self.queries.items()\n",
    "        ]\n",
    "\n",
    "    def save_json(self, path: str):\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(self.dict(), f, indent=2)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_json(cls, path: str):\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        return cls(**data)\n",
    "\n",
    "def generate_question_context_pairs(documents:List[Document],\n",
    "                                    llm,\n",
    "                                    qa_generate_prompt_tmpl,\n",
    "                                    num_questions_per_chunk:int=5):\n",
    "    doc_dict = {doc.metadata[\"doc_id\"]:doc.page_content for doc in documents}\n",
    "    queries = {}\n",
    "    relevant_docs = {}\n",
    "    for doc_id, text in tqdm.tqdm(doc_dict.items()):\n",
    "        query = qa_generate_prompt_tmpl.format(\n",
    "            context_str=text,\n",
    "            num_questions_per_chunk=num_questions_per_chunk,\n",
    "        )\n",
    "        response = llm.invoke(query).content\n",
    "        print(response)\n",
    "        result = re.split(r\"\\n+\",response.strip())\n",
    "        print(result)\n",
    "        questions = [\n",
    "            re.sub(r\"^\\d+[\\).\\s]\", \"\",question).strip() for question in result\n",
    "        ]\n",
    "\n",
    "        questions = [question for question in questions if len(question) > 0][:num_questions_per_chunk]\n",
    "        for question in questions:\n",
    "            question_id = str(uuid.uuid4())\n",
    "            queries[question_id] = question\n",
    "            relevant_docs[question_id] = doc_id\n",
    "\n",
    "    return QuestionContextEvalDataset(\n",
    "        queries=queries,\n",
    "        corpus=doc_dict,\n",
    "        relevant_docs=relevant_docs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1004 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1004 [00:02<48:25,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What does the letter \"W\" represent in the context provided?  \n",
      "2. How might the context of \"W\" be interpreted in different scenarios?\n",
      "['1. What does the letter \"W\" represent in the context provided?  ', '2. How might the context of \"W\" be interpreted in different scenarios?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1004 [00:04<38:28,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What does the letter \"h\" represent in the context provided?  \n",
      "2. How might the single character \"h\" be interpreted in different contexts?\n",
      "['1. What does the letter \"h\" represent in the context provided?  ', '2. How might the single character \"h\" be interpreted in different contexts?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1004 [00:05<27:36,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What does the letter \"a\" represent in the context provided?  \n",
      "2. How might the single character \"a\" be interpreted in different contexts?\n",
      "['1. What does the letter \"a\" represent in the context provided?  ', '2. How might the single character \"a\" be interpreted in different contexts?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1004 [00:06<23:59,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What does the letter \"t\" represent in the context provided?  \n",
      "2. How might the single character \"t\" be interpreted in different contexts?\n",
      "['1. What does the letter \"t\" represent in the context provided?  ', '2. How might the single character \"t\" be interpreted in different contexts?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1004 [00:07<21:03,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What does the letter \"I\" represent in the context provided?  \n",
      "2. How might the single letter \"I\" be interpreted in different contexts or fields?\n",
      "['1. What does the letter \"I\" represent in the context provided?  ', '2. How might the single letter \"I\" be interpreted in different contexts or fields?']\n"
     ]
    }
   ],
   "source": [
    "qa_pairs = generate_question_context_pairs(\n",
    "    documents=chunks,\n",
    "    llm=llm,\n",
    "    qa_generate_prompt_tmpl=DEFAULT_QA_GENERATE_PROMPT_TMPL,\n",
    "    num_questions_per_chunk=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hit_rage(expected_ids,retrieved_ids):\n",
    "    is_hit = any(id in expected_ids for id in retrieved_ids)\n",
    "    return 1.0 if is_hit else 0.0\n",
    "\n",
    "def compute_mrr(expected_ids,retrieved_ids):\n",
    "    if i,id in enumerate(retrieved_ids):\n",
    "        if id in expected_ids:\n",
    "            return 1/(i+1)\n",
    "    return 0.0\n",
    "\n",
    "def compute_ndcg(expected_ids,retrieved_ids):\n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "    for i,id in enumerate(retrieved_ids):\n",
    "        if id in expected_ids:\n",
    "            dcg += 1.0/(i+1)\n",
    "        idcg += 1.0/(i+1)\n",
    "    return dcg/idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_queries(dataset):\n",
    "    values = []\n",
    "    for value in dataset.queries.values():\n",
    "        values.append(value)\n",
    "    return values\n",
    "\n",
    "def extract_doc_ids(documents_):\n",
    "    doc_ids = []\n",
    "    for doc in documents_:\n",
    "        doc_ids.append(doc.metadata[\"doc_id\"])\n",
    "    return doc_ids\n",
    "\n",
    "def evaluate(retriever,dataset):\n",
    "    hit_rate_result = []\n",
    "    ndcg = []\n",
    "    mrr = []\n",
    "    for i in tqdm.tqdm(range(len(dataset.queries))):\n",
    "        context = retriever.invoke(extract_queries(dataset)[i])\n",
    "        \n",
    "        expected_ids = dataset.relevant_docs[List(dataset.queries.keys())[i]]\n",
    "        retrieved_ids = extract_doc_ids(context)\n",
    "        mrr = compute_mrr(expected_ids,retrieved_ids)\n",
    "        ndcg = compute_ndcg(expected_ids,retrieved_ids)\n",
    "        hit_rate_result.append(compute_hit_rage(expected_ids,retrieved_ids))\n",
    "        ndcg.append(ndcg)\n",
    "        mrr.append(mrr)\n",
    "        array = np.array([mrr,hit_rate_result,ndcg])\n",
    "        mean_result = np.mean(array)\n",
    "        mean_result.index = [\"MRR\",\"Hit Rate\",\"NDCG\"]\n",
    "        return mean_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_bm25_rerank_results = evaluate(embedding_bm25_retriever_rerank,qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_embedding_bm25_rerank_results = evaluate(contextual_embedding_bm25_retriever_rerank,qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_retriever_results = evaluate(embedding_retriever,qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_embedding_retriever_results = evaluate(contextual_embedding_retriever,qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_results = evaluate(bm25_retriever,qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_bm25_retriever_results = evaluate(contextual_bm25_retriever,qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(name,eval_result):\n",
    "    metrics = ['MRR','Hit Rate','NDCG']\n",
    "    \n",
    "    columns = [\"Retriever\": [name],\n",
    "               **{metric: val for metric,val in zip(metrics,eval_result.values)}]\n",
    "    metrics_df = pd.DataFrame(columns)\n",
    "    return metrics_df\n",
    "\n",
    "pd.concat([display_results(\"embedding retriever\",embedding_retriever_results),\n",
    "           display_results(\"embedding bm25 retriever\",embedding_bm25_rerank_results),\n",
    "           display_results(\"Contextual embedding retriever\",contextual_embedding_retriever_results),]\n",
    "           ,ignore_index=True,axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
